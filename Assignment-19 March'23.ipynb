{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce93b064-42d0-4f9e-97e0-23aeff58c073",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9703a2-3e42-4058-a6f4-1465ec78a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling, also known as Min-Max normalization or feature scaling, is a data preprocessing technique used to transform numerical features in a dataset into a specific range, typically between 0 and 1. It rescales the values of the features so that they are all proportionally adjusted to fit within the specified range. This can be helpful when dealing with machine learning algorithms that are sensitive to the scale of input features, such as gradient descent-based algorithms and distance-based methods like k-nearest neighbors.\n",
    "\n",
    "The Min-Max scaling formula for a feature \"x\" in a dataset is as follows:\n",
    "\n",
    "�\n",
    "new\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "new\n",
    "X \n",
    "new\n",
    "​\n",
    "  is the rescaled value of the feature \"X.\"\n",
    "�\n",
    "X is the original value of the feature \"X.\"\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value of feature \"X\" in the dataset.\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value of feature \"X\" in the dataset.\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset of exam scores with values ranging from 60 to 100, and you want to apply Min-Max scaling to these scores to make them fall within the range of 0 to 1.\n",
    "\n",
    "Original Exam Scores:\n",
    "\n",
    "Student 1: 60\n",
    "Student 2: 80\n",
    "Student 3: 100\n",
    "Calculate \n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  and \n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    " :\n",
    "\n",
    "�\n",
    "min\n",
    "=\n",
    "60\n",
    "X \n",
    "min\n",
    "​\n",
    " =60\n",
    "�\n",
    "max\n",
    "=\n",
    "100\n",
    "X \n",
    "max\n",
    "​\n",
    " =100\n",
    "Apply the Min-Max scaling formula for each student's score:\n",
    "\n",
    "Student 1: \n",
    "�\n",
    "new\n",
    "=\n",
    "60\n",
    "−\n",
    "60\n",
    "100\n",
    "−\n",
    "60\n",
    "=\n",
    "0.0\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "100−60\n",
    "60−60\n",
    "​\n",
    " =0.0\n",
    "Student 2: \n",
    "�\n",
    "new\n",
    "=\n",
    "80\n",
    "−\n",
    "60\n",
    "100\n",
    "−\n",
    "60\n",
    "=\n",
    "0.4\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "100−60\n",
    "80−60\n",
    "​\n",
    " =0.4\n",
    "Student 3: \n",
    "�\n",
    "new\n",
    "=\n",
    "100\n",
    "−\n",
    "60\n",
    "100\n",
    "−\n",
    "60\n",
    "=\n",
    "1.0\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "100−60\n",
    "100−60\n",
    "​\n",
    " =1.0\n",
    "After Min-Max scaling, the rescaled scores for the students now fall within the range of 0 to 1:\n",
    "\n",
    "Student 1: 0.0\n",
    "Student 2: 0.4\n",
    "Student 3: 1.0\n",
    "Min-Max scaling is a simple but effective technique to ensure that all features in your dataset are on a similar scale, which can improve the performance of many machine learning algorithms and make it easier to compare and interpret the significance of different features in your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2583aca-184a-4a76-87dc-eb7d8b264fd9",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f9b02-4bf5-43e2-a798-ccd36fdd2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique, also known as vector normalization or L2 normalization, is a feature scaling method used to transform numerical features in a dataset into unit vectors. In this technique, each data point (or feature vector) is scaled such that its Euclidean norm (L2 norm) becomes equal to 1. This means that the data points are rescaled to have a length of 1 while preserving their direction. Unit Vector scaling is particularly useful when you want to emphasize the direction or orientation of data points while making their magnitudes consistent.\n",
    "\n",
    "The formula for scaling a feature vector \"X\" into a unit vector is as follows:\n",
    "\n",
    "�\n",
    "new\n",
    "=\n",
    "�\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "∥X∥ \n",
    "2\n",
    "​\n",
    " \n",
    "X\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "new\n",
    "X \n",
    "new\n",
    "​\n",
    "  is the unit vector representing the scaled feature.\n",
    "�\n",
    "X is the original feature vector.\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "∥X∥ \n",
    "2\n",
    "​\n",
    "  is the L2 norm or Euclidean norm of the feature vector, which is calculated as the square root of the sum of the squares of its components.\n",
    "Here's an example to illustrate Unit Vector scaling:\n",
    "\n",
    "Suppose you have a dataset of 2D points, and you want to scale these points into unit vectors while preserving their direction. Consider the following original points:\n",
    "\n",
    "Original 2D Points:\n",
    "\n",
    "Point A: (3, 4)\n",
    "Point B: (1, 2)\n",
    "Point C: (6, 8)\n",
    "Calculate the L2 norm for each point:\n",
    "\n",
    "L2 norm of Point A: \n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "=\n",
    "3\n",
    "2\n",
    "+\n",
    "4\n",
    "2\n",
    "=\n",
    "5\n",
    "∥A∥ \n",
    "2\n",
    "​\n",
    " = \n",
    "3 \n",
    "2\n",
    " +4 \n",
    "2\n",
    " \n",
    "​\n",
    " =5\n",
    "L2 norm of Point B: \n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "=\n",
    "1\n",
    "2\n",
    "+\n",
    "2\n",
    "2\n",
    "=\n",
    "5\n",
    "∥B∥ \n",
    "2\n",
    "​\n",
    " = \n",
    "1 \n",
    "2\n",
    " +2 \n",
    "2\n",
    " \n",
    "​\n",
    " = \n",
    "5\n",
    "​\n",
    " \n",
    "L2 norm of Point C: \n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "=\n",
    "6\n",
    "2\n",
    "+\n",
    "8\n",
    "2\n",
    "=\n",
    "10\n",
    "∥C∥ \n",
    "2\n",
    "​\n",
    " = \n",
    "6 \n",
    "2\n",
    " +8 \n",
    "2\n",
    " \n",
    "​\n",
    " =10\n",
    "Apply the Unit Vector scaling formula for each point:\n",
    "\n",
    "Unit Vector for Point A: \n",
    "(\n",
    "3\n",
    "5\n",
    ",\n",
    "4\n",
    "5\n",
    ")\n",
    "( \n",
    "5\n",
    "3\n",
    "​\n",
    " , \n",
    "5\n",
    "4\n",
    "​\n",
    " )\n",
    "Unit Vector for Point B: \n",
    "(\n",
    "1\n",
    "5\n",
    ",\n",
    "2\n",
    "5\n",
    ")\n",
    "( \n",
    "5\n",
    "​\n",
    " \n",
    "1\n",
    "​\n",
    " , \n",
    "5\n",
    "​\n",
    " \n",
    "2\n",
    "​\n",
    " )\n",
    "Unit Vector for Point C: \n",
    "(\n",
    "6\n",
    "10\n",
    ",\n",
    "8\n",
    "10\n",
    ")\n",
    "=\n",
    "(\n",
    "3\n",
    "5\n",
    ",\n",
    "4\n",
    "5\n",
    ")\n",
    "( \n",
    "10\n",
    "6\n",
    "​\n",
    " , \n",
    "10\n",
    "8\n",
    "​\n",
    " )=( \n",
    "5\n",
    "3\n",
    "​\n",
    " , \n",
    "5\n",
    "4\n",
    "​\n",
    " )\n",
    "After Unit Vector scaling, the points are transformed into unit vectors with lengths equal to 1, preserving their original directions:\n",
    "\n",
    "Unit Vector for Point A: \n",
    "(\n",
    "0.6\n",
    ",\n",
    "0.8\n",
    ")\n",
    "(0.6,0.8)\n",
    "Unit Vector for Point B: \n",
    "(\n",
    "0.447\n",
    ",\n",
    "0.894\n",
    ")\n",
    "(0.447,0.894)\n",
    "Unit Vector for Point C: \n",
    "(\n",
    "0.6\n",
    ",\n",
    "0.8\n",
    ")\n",
    "(0.6,0.8)\n",
    "Differences between Min-Max Scaling and Unit Vector Scaling:\n",
    "\n",
    "Range:\n",
    "\n",
    "Min-Max Scaling: Scales features to a specific range (e.g., 0 to 1).\n",
    "Unit Vector Scaling: Scales features to have a length of 1, preserving their direction.\n",
    "Magnitude:\n",
    "\n",
    "Min-Max Scaling adjusts the magnitude of features to a common range.\n",
    "Unit Vector Scaling focuses on the direction of features while maintaining a consistent magnitude.\n",
    "Use cases:\n",
    "\n",
    "Min-Max Scaling is often used when you want to make features comparable by bringing them to the same scale.\n",
    "Unit Vector Scaling is useful when you want to emphasize the direction of data points, especially in applications like text analysis or recommendation systems where the magnitude of features is less important compared to their orientation.\n",
    "\n",
    "\n",
    "\n",
    "Is this conversation helpful so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9944549-c71d-4d12-8793-b1ba37ed92eb",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032d1c4-dc8d-4494-b7ec-12276bbcb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used in data analysis and machine learning. Its primary goal is to reduce the dimensionality of a dataset while retaining as much of the essential variation in the data as possible. PCA achieves this by transforming the original features into a new set of orthogonal, uncorrelated variables called principal components.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "Standardize the data: If the features in the dataset are measured on different scales, it's essential to standardize them (mean = 0, standard deviation = 1) to ensure that all features contribute equally to the PCA.\n",
    "\n",
    "Calculate the covariance matrix: The covariance matrix represents the relationships and dependencies between the features in the dataset.\n",
    "\n",
    "Compute the eigenvalues and eigenvectors of the covariance matrix: The eigenvalues represent the variance of the data along the principal components, and the eigenvectors represent the directions of these principal components.\n",
    "\n",
    "Select a subset of the principal components: Typically, you can choose a subset of the principal components based on their corresponding eigenvalues. The larger the eigenvalue, the more variance is explained by the corresponding principal component.\n",
    "\n",
    "Project the data onto the selected principal components: This transformation reduces the dimensionality of the data by representing each data point in the new coordinate system defined by the selected principal components.\n",
    "\n",
    "Here's an example to illustrate PCA's application:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Height\" and \"Weight,\" and you want to reduce the dimensionality of the dataset while preserving as much information as possible. The data looks like this:\n",
    "\n",
    "Original Data:\n",
    "\n",
    "Sample\tHeight (cm)\tWeight (kg)\n",
    "1\t170\t68\n",
    "2\t165\t55\n",
    "3\t180\t80\n",
    "4\t175\t72\n",
    "Standardize the data (subtract the mean and divide by the standard deviation for each feature).\n",
    "\n",
    "Calculate the covariance matrix of the standardized data:\n",
    "\n",
    "[\n",
    "0.5\n",
    "0.6\n",
    "0.6\n",
    "0.7\n",
    "]\n",
    "[ \n",
    "0.5\n",
    "0.6\n",
    "​\n",
    "  \n",
    "0.6\n",
    "0.7\n",
    "​\n",
    " ]\n",
    "Compute the eigenvalues and eigenvectors of the covariance matrix. Let's say you find two eigenvalues: λ1 = 1.0 and λ2 = 0.2, and their corresponding eigenvectors are [0.8, 0.6] and [-0.6, 0.8], respectively.\n",
    "\n",
    "Select the principal components: You may decide to keep both principal components because they collectively explain most of the variance. You can also set a threshold (e.g., retaining principal components that explain 95% of the variance).\n",
    "\n",
    "Project the data onto the selected principal components:\n",
    "\n",
    "Projected Data:\n",
    "\n",
    "Sample\tPrincipal Component 1\tPrincipal Component 2\n",
    "1\t0.2\t-0.4\n",
    "2\t-0.4\t0.2\n",
    "3\t0.4\t0.6\n",
    "4\t-0.2\t-0.4\n",
    "Now you have reduced the data from two dimensions (Height and Weight) to two new dimensions (Principal Component 1 and Principal Component 2) while retaining the essential information in the data. This reduction can be particularly useful in cases where you have high-dimensional data and want to simplify it for analysis or visualization without losing significant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678f9cb-1e3f-429e-b114-0423d7f2bba7",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b243fa-2dd9-48b8-95c2-35c1017b9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA (Principal Component Analysis) is often used for feature extraction in data analysis and machine learning. Feature extraction is the process of transforming the original features of a dataset into a new set of features that capture the most important information while reducing dimensionality. PCA is a specific technique for feature extraction that focuses on finding linear combinations of the original features, called principal components, to represent the data more effectively.\n",
    "\n",
    "The relationship between PCA and feature extraction can be summarized as follows:\n",
    "\n",
    "PCA as a Feature Extraction Method:\n",
    "PCA identifies the directions in the data (the principal components) that capture the most significant variance.\n",
    "These principal components can be used as new features that often provide a more compact and informative representation of the data.\n",
    "The number of principal components chosen determines the dimensionality of the feature space after extraction.\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose you have a dataset with six features related to student performance: \"Math Score,\" \"English Score,\" \"Science Score,\" \"Attendance Percentage,\" \"Number of Study Hours,\" and \"Parental Income.\" You want to extract a reduced set of features that capture the essential information while reducing dimensionality.\n",
    "\n",
    "Original Data:\n",
    "\n",
    "Student\tMath Score\tEnglish Score\tScience Score\tAttendance Percentage\tStudy Hours\tParental Income\n",
    "1\t85\t78\t90\t92\t4\t45000\n",
    "2\t92\t85\t88\t95\t5\t60000\n",
    "3\t78\t75\t82\t90\t3\t35000\n",
    "4\t88\t80\t85\t93\t4\t55000\n",
    "Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "Apply PCA to the standardized data to find the principal components. Let's say you find three principal components, PC1, PC2, and PC3, which capture the most variance.\n",
    "\n",
    "Project the original data onto these principal components to create the reduced feature set.\n",
    "\n",
    "Reduced Feature Set:\n",
    "\n",
    "Student\tPC1\tPC2\tPC3\n",
    "1\t1.05\t0.11\t-0.18\n",
    "2\t0.69\t0.03\t0.35\n",
    "3\t-1.04\t-0.02\t-0.26\n",
    "4\t-0.69\t-0.11\t0.09\n",
    "In this example, PCA has reduced the dimensionality of the data from six original features to three new features (the principal components). These new features are linear combinations of the original features and capture most of the variance in the data. The reduced feature set can be used for subsequent analysis, such as machine learning modeling, and is often more interpretable and computationally efficient than the original high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0749636-17ef-4581-9595-8b86becb62c4",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a378a-3fee-413b-b3af-bc5c42404e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling on the features such as price, rating, and delivery time. Min-Max scaling will transform these features to a common range, typically between 0 and 1, so that they can be used effectively in the recommendation system without certain features dominating others due to their different scales. Here's how you can use Min-Max scaling for this purpose:\n",
    "\n",
    "Identify the Features:\n",
    "\n",
    "In your dataset, you have features like price, rating, and delivery time that you want to scale using Min-Max scaling.\n",
    "Calculate the Minimum and Maximum Values for Each Feature:\n",
    "\n",
    "For each feature (price, rating, and delivery time), calculate the minimum (X_min) and maximum (X_max) values within your dataset. This is done by finding the smallest and largest values for each feature across all the data points.\n",
    "Apply Min-Max Scaling:\n",
    "\n",
    "For each data point, apply the Min-Max scaling formula to transform the feature values to the 0 to 1 range.\n",
    "Min-Max Scaling Formula:\n",
    "�\n",
    "new\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "�\n",
    "new\n",
    "X \n",
    "new\n",
    "​\n",
    "  is the rescaled value of the feature.\n",
    "�\n",
    "X is the original value of the feature.\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value of the feature.\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value of the feature.\n",
    "Repeat the Min-Max Scaling Process for Each Feature:\n",
    "\n",
    "Apply the Min-Max scaling process separately to each of the features (price, rating, and delivery time).\n",
    "Store the Scaled Features:\n",
    "\n",
    "Replace the original feature values with the scaled values in your dataset.\n",
    "Once you've completed this Min-Max scaling process, your features (price, rating, and delivery time) will be transformed to a common scale, ensuring that no single feature dominates the others due to differences in their original value ranges. This preprocessing step is crucial for building a recommendation system because it allows you to treat all features equally when calculating recommendations, ensuring that price, rating, and delivery time have balanced importance in the recommendation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a93e99-45a9-4dfe-bb40-22099b0ad65b",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6fb8b-622a-4677-b18c-f9e2a9ae7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using PCA (Principal Component Analysis) to reduce the dimensionality of a dataset in the context of building a stock price prediction model can be a valuable approach, especially when dealing with a dataset that contains a large number of features. Reducing dimensionality can simplify the modeling process, reduce computational complexity, and potentially improve model performance. Here's how you can use PCA for this purpose:\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Start by preprocessing the dataset. This typically involves tasks like handling missing data, standardizing the features (mean = 0, standard deviation = 1), and dealing with any outliers.\n",
    "Feature Selection:\n",
    "\n",
    "Before applying PCA, carefully select the features that are relevant for predicting stock prices. Not all features may contribute significantly to the predictive power of the model. Eliminate irrelevant or redundant features to reduce noise and improve the efficiency of PCA.\n",
    "Apply PCA:\n",
    "\n",
    "After feature selection, apply PCA to the remaining features. PCA will identify linear combinations of the original features (principal components) that capture the most significant variance in the data.\n",
    "Specify the number of principal components to retain. This decision can be based on different criteria, such as the cumulative explained variance (e.g., retaining enough components to explain a high percentage, like 95%, of the total variance) or domain knowledge.\n",
    "Calculate Principal Components:\n",
    "\n",
    "PCA will provide you with the principal components, which are linear combinations of the original features. These components are orthogonal and ordered by the amount of variance they capture, with the first principal component capturing the most variance.\n",
    "Project Data onto Principal Components:\n",
    "\n",
    "Transform the original dataset by projecting it onto the selected principal components. This creates a reduced-dimension representation of the data, with each data point expressed in terms of the principal components.\n",
    "Reduced-Dimension Dataset:\n",
    "\n",
    "The transformed dataset now contains fewer features, which are the principal components. These components are uncorrelated and ordered by importance in explaining the variance in the data.\n",
    "Model Building:\n",
    "\n",
    "Use the reduced-dimension dataset as input for your stock price prediction model. You can use regression techniques or time series forecasting methods, depending on the nature of your problem.\n",
    "Benefits of using PCA for stock price prediction:\n",
    "\n",
    "Dimensionality Reduction: PCA reduces the number of features, making it easier to work with the data and potentially improving model efficiency.\n",
    "\n",
    "Noise Reduction: By focusing on the most significant sources of variation in the data, PCA can help reduce the impact of noise in the dataset.\n",
    "\n",
    "Multicollinearity: PCA can address multicollinearity issues by providing uncorrelated principal components.\n",
    "\n",
    "Interpretability: Reduced-dimension data can be easier to interpret, and the most important features (principal components) can provide insights into the factors driving stock price movements.\n",
    "\n",
    "It's important to note that PCA may not always be the best choice, and the effectiveness of dimensionality reduction techniques depends on the specific characteristics of your dataset and problem. You should also evaluate the impact of PCA on model performance and carefully interpret the results to ensure they align with your domain knowledge and objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6297a10-d06f-4ebe-bdcf-4884a6cc6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824169d5-0dc6-4dd4-8cd1-7c21b04d68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform Min-Max scaling to transform a dataset of values to a range of -1 to 1, you need to calculate the minimum and maximum values in the dataset and then use the Min-Max scaling formula. Here are the steps to achieve this:\n",
    "\n",
    "Identify the dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "Calculate the minimum and maximum values in the dataset:\n",
    "\n",
    "Minimum value (\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    " ) = 1\n",
    "Maximum value (\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    " ) = 20\n",
    "Apply the Min-Max scaling formula to each value in the dataset:\n",
    "\n",
    "�\n",
    "new\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Scale each value in the dataset using the formula:\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "1\n",
    "X=1:\n",
    "�\n",
    "new\n",
    "=\n",
    "1\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "0\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "20−1\n",
    "1−1\n",
    "​\n",
    " =0\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "5\n",
    "X=5:\n",
    "�\n",
    "new\n",
    "=\n",
    "5\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "0.2\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "20−1\n",
    "5−1\n",
    "​\n",
    " =0.2\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "10\n",
    "X=10:\n",
    "�\n",
    "new\n",
    "=\n",
    "10\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "0.45\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "20−1\n",
    "10−1\n",
    "​\n",
    " =0.45\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "15\n",
    "X=15:\n",
    "�\n",
    "new\n",
    "=\n",
    "15\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "0.7\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "20−1\n",
    "15−1\n",
    "​\n",
    " =0.7\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "20\n",
    "X=20:\n",
    "�\n",
    "new\n",
    "=\n",
    "20\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "1\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "20−1\n",
    "20−1\n",
    "​\n",
    " =1\n",
    "\n",
    "After performing Min-Max scaling, the values in the dataset are transformed to the range of -1 to 1 as follows:\n",
    "\n",
    "Transformed Dataset: [-1, -0.6, -0.1, 0.3, 1]\n",
    "\n",
    "Now, the values have been scaled to fit within the specified range, with -1 representing the minimum value in the dataset, and 1 representing the maximum value, while the other values are proportionally adjusted in between."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f69454-bb27-4e20-b9bd-0fdb9f3e4015",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b9aea-6bd7-44ce-8cc2-ba855bcecf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The decision on how many principal components to retain during PCA for feature extraction depends on several factors, including the specific goals of your analysis, the variance explained by each principal component, and the trade-off between dimensionality reduction and information preservation. Let's discuss some considerations that can guide your choice of how many principal components to retain:\n",
    "\n",
    "Explained Variance:\n",
    "\n",
    "One common approach is to examine the cumulative explained variance. You can calculate the cumulative variance explained by the principal components and decide how much variance you want to retain. A common threshold is to retain enough principal components to explain a high percentage (e.g., 95% or 99%) of the total variance in the data.\n",
    "You can calculate the cumulative explained variance as you iterate through the principal components, and once it reaches the desired threshold, you stop adding more components.\n",
    "Scree Plot:\n",
    "\n",
    "Another useful method is to create a scree plot, which shows the explained variance for each principal component. The point where the explained variance starts to level off can be a good indication of how many components to retain.\n",
    "You typically look for an \"elbow\" point in the plot, which suggests diminishing returns in explained variance beyond that point.\n",
    "Domain Knowledge:\n",
    "\n",
    "Consider the domain-specific context of your dataset. Are there features that are known to be less relevant or less important for your analysis? This can guide your decision on which components to retain.\n",
    "Trade-off:\n",
    "\n",
    "There is a trade-off between dimensionality reduction and information loss. Retaining fewer components leads to a more compact representation of the data but may result in some loss of information. It's important to strike a balance that best serves your modeling and analysis needs.\n",
    "Computational Resources:\n",
    "\n",
    "Depending on the size of your dataset, reducing dimensionality can also have computational advantages. Fewer components mean faster training and lower resource requirements for machine learning models.\n",
    "In the case of your dataset containing features like height, weight, age, gender, and blood pressure, it's challenging to make a specific recommendation without more information about the dataset and its context. However, you can follow these general steps to decide how many principal components to retain:\n",
    "\n",
    "Standardize the features, ensuring that they have zero mean and unit variance.\n",
    "\n",
    "Calculate the covariance matrix of the standardized features.\n",
    "\n",
    "Compute the eigenvalues and eigenvectors of the covariance matrix to determine the principal components.\n",
    "\n",
    "Calculate the explained variance for each principal component.\n",
    "\n",
    "Analyze the cumulative explained variance and potentially create a scree plot to identify an appropriate threshold.\n",
    "\n",
    "Consider domain knowledge and the trade-off between dimensionality reduction and information preservation.\n",
    "\n",
    "Based on these considerations, you can decide how many principal components to retain to achieve the right balance between dimensionality reduction and information retention for your specific analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "950e288b-6ff6-4e96-b8af-88b1349cf55b",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede3073-55bc-4263-81ff-87033fa37b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Linear Support Vector Machine (SVM) is a supervised learning algorithm used for classification tasks. It aims to find the optimal hyperplane that separates classes in a dataset by maximizing the margin between the classes.\n",
    "\n",
    "The mathematical formula for a linear SVM can be represented as follows:\n",
    "\n",
    "Given a dataset with features \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  and corresponding class labels \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  where \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  belongs to \n",
    "�\n",
    "�\n",
    "R \n",
    "n\n",
    "  and \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is either -1 or 1 (for binary classification):\n",
    "\n",
    "The linear SVM classification function can be written as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "sign\n",
    "(\n",
    "�\n",
    "⋅\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "f(x)=sign(w⋅x+b)\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "w is the weight vector perpendicular to the hyperplane.\n",
    "�\n",
    "x is the input feature vector.\n",
    "�\n",
    "b is the bias term (or intercept).\n",
    "⋅\n",
    "⋅ denotes the dot product between vectors.\n",
    "sign\n",
    "(\n",
    "⋅\n",
    ")\n",
    "sign(⋅) is the sign function that returns +1 for positive values, 0 for 0, and -1 for negative values.\n",
    "The decision boundary is represented by the hyperplane defined as \n",
    "�\n",
    "⋅\n",
    "�\n",
    "+\n",
    "�\n",
    "=\n",
    "0\n",
    "w⋅x+b=0, and the distance between the hyperplane and the closest data points (support vectors) is maximized in a process known as margin maximization.\n",
    "\n",
    "In training a linear SVM, the objective is to find the optimal \n",
    "�\n",
    "w and \n",
    "�\n",
    "b by solving an optimization problem that minimizes the norm of \n",
    "�\n",
    "w subject to the constraint that each data point lies on the correct side of the hyperplane with a margin.\n",
    "\n",
    "The optimization problem is formulated using Lagrange multipliers to derive the support vectors, which are the data points that contribute to defining the hyperplane.\n",
    "\n",
    "The linear SVM aims to find the optimal separating hyperplane that maximizes the margin between classes while minimizing classification errors, providing a robust classification boundary for the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe9e95-bc76-4834-a22b-4e08a62d78a6",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fba811-0f3a-4ffd-8e64-a1f790b84c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) involves maximizing the margin between classes while minimizing classification errors. The SVM seeks to find the optimal hyperplane that separates classes in a dataset.\n",
    "\n",
    "The objective function can be formulated as follows:\n",
    "\n",
    "Given a dataset with features \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  and corresponding class labels \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  where \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  belongs to \n",
    "�\n",
    "�\n",
    "R \n",
    "n\n",
    "  and \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is either -1 or 1 (for binary classification):\n",
    "\n",
    "Optimal Margin:\n",
    "\n",
    "The objective is to find the hyperplane that maximizes the margin between classes.\n",
    "The distance between the hyperplane and the closest data points (support vectors) from each class needs to be maximized.\n",
    "Formulation of Objective Function:\n",
    "\n",
    "The objective function of the linear SVM involves minimizing the norm of the weight vector \n",
    "�\n",
    "w while satisfying certain constraints.\n",
    "The objective function for a linear SVM is often expressed as:\n",
    "\n",
    "min\n",
    "⁡\n",
    "�\n",
    ",\n",
    "�\n",
    "1\n",
    "2\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "min \n",
    "w,b\n",
    "​\n",
    "  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " \n",
    "\n",
    "Subject to the constraints:\n",
    "\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "≥\n",
    "1\n",
    "for all training samples \n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "y \n",
    "i\n",
    "​\n",
    " (w⋅x \n",
    "i\n",
    "​\n",
    " +b)≥1for all training samples (x \n",
    "i\n",
    "​\n",
    " ,y \n",
    "i\n",
    "​\n",
    " )\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "w is the weight vector perpendicular to the hyperplane.\n",
    "�\n",
    "b is the bias term (or intercept).\n",
    "⋅\n",
    "⋅ denotes the dot product between vectors.\n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  represents the class labels (-1 or 1) for each sample \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    " .\n",
    "The inequality constraint ensures that each data point lies on the correct side of the hyperplane with a margin of at least 1.\n",
    "The objective is to minimize \n",
    "1\n",
    "2\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " , which corresponds to minimizing the norm of the weight vector \n",
    "�\n",
    "w. This is subject to the constraints that all data points are correctly classified and lie on the correct side of the hyperplane, with a margin of at least 1.\n",
    "\n",
    "The optimization process involves solving this constrained optimization problem, typically using methods such as quadratic programming or convex optimization techniques to find the optimal \n",
    "�\n",
    "w and \n",
    "�\n",
    "b that define the separating hyperplane with the maximum margin while minimizing classification errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42649dc0-249e-4bed-9750-22ae13eea20a",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b05e13-441e-4260-bc50-5d73ba7fb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "The kernel trick in Support Vector Machines (SVMs) is a mathematical method that allows SVMs to implicitly operate in a higher-dimensional space without explicitly transforming the input features into that higher-dimensional space. It enables SVMs to find non-linear decision boundaries in the original feature space by effectively computing the dot products between transformed feature vectors in this higher-dimensional space.\n",
    "\n",
    "Key Points about the Kernel Trick:\n",
    "Handling Non-Linearity:\n",
    "\n",
    "SVMs with a linear kernel work well for linearly separable data. However, many real-world datasets are not linearly separable.\n",
    "The kernel trick allows SVMs to handle non-linearly separable data by transforming it into a higher-dimensional space where the data might be linearly separable.\n",
    "Kernel Functions:\n",
    "\n",
    "Kernel functions compute the dot product between transformed feature vectors in a higher-dimensional space without explicitly mapping the data into that space.\n",
    "Common kernel functions include:\n",
    "Polynomial kernel: \n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "(\n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "�\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=(x \n",
    "i\n",
    "​\n",
    " ⋅x \n",
    "j\n",
    "​\n",
    " +c) \n",
    "d\n",
    " \n",
    "Radial Basis Function (RBF) kernel: \n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "exp\n",
    "⁡\n",
    "(\n",
    "−\n",
    "�\n",
    "∥\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "∥\n",
    "2\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=exp(−γ∥x \n",
    "i\n",
    "​\n",
    " −x \n",
    "j\n",
    "​\n",
    " ∥ \n",
    "2\n",
    " )\n",
    "Sigmoid kernel, among others.\n",
    "Avoiding Explicit Transformation:\n",
    "\n",
    "Instead of explicitly transforming the input features into a higher-dimensional space, the kernel trick computes the dot product between the transformed feature vectors directly in the original feature space.\n",
    "Benefits:\n",
    "\n",
    "Avoids the computational cost and memory requirements of explicitly transforming the data into a higher-dimensional space, especially for high-dimensional or infinite-dimensional spaces.\n",
    "Enables SVMs to learn complex, non-linear decision boundaries by implicitly utilizing the higher-dimensional space.\n",
    "Flexibility in Finding Decision Boundaries:\n",
    "\n",
    "The kernel trick allows SVMs to find decision boundaries that are non-linear in the original feature space by effectively mapping the data to a higher-dimensional space where a linear separation might exist.\n",
    "Conclusion:\n",
    "In essence, the kernel trick in SVMs allows for the implicit transformation of data into a higher-dimensional space via kernel functions. By operating in this higher-dimensional space, SVMs can find decision boundaries that are non-linear in the original feature space, providing flexibility in handling non-linearly separable datasets without the need for explicit feature transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b245185-6bf6-4f02-bdf7-b9c8e4b566ed",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296a5aa-bda8-4fb6-b423-988b7eb33623",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Support Vector Machines (SVMs), support vectors play a crucial role in defining the decision boundary and determining the optimal hyperplane. They are the data points that lie closest to the decision boundary or hyperplane and have a non-zero value for the Lagrange multiplier (also known as the 'alpha' value) in the optimization process.\n",
    "\n",
    "Role of Support Vectors in SVM:\n",
    "Defining the Decision Boundary:\n",
    "\n",
    "Support vectors are the critical data points that directly influence the position and orientation of the hyperplane.\n",
    "They define the margins (or the maximum width) of the separation between classes, determining the decision boundary.\n",
    "Alpha Values:\n",
    "\n",
    "During the training phase of SVM, the optimization algorithm identifies and selects the support vectors.\n",
    "Each support vector has a corresponding non-zero alpha value, while other data points have alpha values close to zero.\n",
    "The non-zero alpha values correspond to the importance or weight assigned to the support vectors in defining the hyperplane.\n",
    "Formulating the Hyperplane:\n",
    "\n",
    "The hyperplane is entirely determined by the support vectors.\n",
    "The decision boundary is constructed as a linear combination of the support vectors, weighted by their alpha values.\n",
    "Example Illustration:\n",
    "Consider a simple example with two classes (red circles and blue triangles) that are not linearly separable in 2D space:\n",
    "\n",
    "SVM aims to find the optimal hyperplane that separates these classes while maximizing the margin between them.\n",
    "\n",
    "In the 2D feature space, the hyperplane would be a line. However, to achieve separation, the data is implicitly mapped to a higher-dimensional space using a kernel function (e.g., polynomial, RBF).\n",
    "\n",
    "Support vectors are the data points closest to the decision boundary or within the margin, which could be a few red circles and blue triangles.\n",
    "\n",
    "The decision boundary (hyperplane) is determined by these support vectors. The other data points that are farther away from the boundary in the higher-dimensional space have alpha values close to zero and do not significantly affect the hyperplane's position.\n",
    "\n",
    "Conclusion:\n",
    "Support vectors are crucial elements in SVMs that determine the position and orientation of the decision boundary. They play a pivotal role in defining the hyperplane and maximizing the margin between classes, allowing SVMs to effectively separate non-linearly separable datasets in higher-dimensional spaces while relying on a subset of critical data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb2018-0bfd-4501-83c5-c127c5b3eff1",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa6ecb-c083-4399-a2c6-d665d914960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! To illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines (SVMs), let's use visual examples in a two-dimensional feature space.\n",
    "\n",
    "Hyperplane:\n",
    "Definition: The hyperplane is the decision boundary that separates classes in an SVM.\n",
    "Example: In a binary classification scenario, it's a line in 2D separating two classes.\n",
    "Graph:\n",
    "Hyperplane\n",
    "\n",
    "Marginal Plane:\n",
    "Definition: The marginal planes are parallel planes parallel to the hyperplane, touching the support vectors.\n",
    "Example: For a binary classification problem, they run parallel to the hyperplane and touch the closest data points (support vectors).\n",
    "Graph:\n",
    "Marginal Plane\n",
    "\n",
    "Soft Margin:\n",
    "Definition: Soft margin SVM allows some misclassifications by introducing a margin of tolerance for classification errors.\n",
    "Example: When the data is not perfectly separable, a soft margin SVM permits misclassifications while still finding a separating hyperplane.\n",
    "Graph:\n",
    "Soft Margin\n",
    "\n",
    "Hard Margin:\n",
    "Definition: Hard margin SVM aims to find a hyperplane that perfectly separates the classes without allowing any misclassifications.\n",
    "Example: Suitable for linearly separable data with no overlap, where a clear boundary can be drawn.\n",
    "Graph:\n",
    "Hard Margin\n",
    "\n",
    "Explanation:\n",
    "Hyperplane is the decision boundary that separates classes.\n",
    "Marginal Plane includes the hyperplane and extends to the support vectors, defining the margin.\n",
    "Soft Margin allows for some misclassifications (with a penalty) to find a feasible boundary in case of non-linearly separable data.\n",
    "Hard Margin strictly enforces no misclassifications, suitable for linearly separable data.\n",
    "In practice, the choice between a soft margin (allowing misclassifications) and a hard margin (no misclassifications) depends on the dataset's characteristics, aiming for a balance between avoiding overfitting (soft margin) and having a more generalizable model (hard margin). Graphical representations help visualize how these concepts manifest in SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c1f05-e6c4-4701-a13f-2eef719ffc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7afba-2f5a-4f78-bf6c-8f5a75bd1994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

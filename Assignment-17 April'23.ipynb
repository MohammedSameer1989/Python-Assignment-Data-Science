{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb516c10-568b-4b62-8cb4-19370b308dae",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca255823-5153-4b1a-9242-fef54d444904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks that combines the concepts of gradient descent optimization and boosting algorithms to create a strong predictive model. It builds an ensemble of weak prediction models, typically decision trees, sequentially, aiming to minimize a loss function and fit the data more accurately.\n",
    "\n",
    "Key Components of Gradient Boosting Regression:\n",
    "Weak Learners (Base Models):\n",
    "\n",
    "Gradient Boosting Regression typically uses decision trees as weak learners (or base models). These are often shallow trees, known as decision stumps, to avoid overfitting.\n",
    "Gradient Descent Optimization:\n",
    "\n",
    "At its core, Gradient Boosting Regression minimizes a loss function (like mean squared error for regression tasks) using gradient descent.\n",
    "It works by optimizing the residuals (errors) made by the previous model in the ensemble.\n",
    "Sequential Model Building:\n",
    "\n",
    "Models are built sequentially, and each new model focuses on reducing the errors made by the ensemble of existing models.\n",
    "The subsequent models fit the residuals (negative gradients of the loss function) of the previous models, aiming to minimize the overall loss.\n",
    "Gradient Calculation:\n",
    "\n",
    "Gradient Boosting Regression computes the gradient (derivatives) of the loss function with respect to the predictions made by the existing ensemble of models.\n",
    "This gradient information guides the creation of the next model in the sequence.\n",
    "Ensemble Prediction:\n",
    "\n",
    "Predictions from all weak learners are combined by weighted averaging to produce the final prediction of the ensemble model.\n",
    "Advantages of Gradient Boosting Regression:\n",
    "Handles complex nonlinear relationships in data effectively.\n",
    "Less prone to overfitting compared to some other algorithms if properly tuned.\n",
    "Provides feature importance information.\n",
    "Example Implementations:\n",
    "scikit-learn library in Python provides an implementation of Gradient Boosting Regression through the GradientBoostingRegressor class.\n",
    "Conclusion:\n",
    "Gradient Boosting Regression is a powerful technique for regression tasks that builds an ensemble of weak models sequentially to create a robust and accurate predictive model by minimizing the error or loss function. It is widely used in various domains due to its capability to handle complex data and produce high-quality predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e5fd24-02e8-49e2-82f9-a14a9d0fa5e6",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b98c40a-c87a-451d-8620-0b854b4fedc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 122.66742050800201\n",
      "R-squared (R2): -2.5805096524147326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_355/2307291938.py:37: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  predictions = np.sum(self.learning_rate * model.predict(X) for model in self.models)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X[:, 0] + np.random.randn(100)  # Linear relationship with added noise\n",
    "\n",
    "# Gradient Boosting Regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with mean for first prediction\n",
    "        initial_prediction = np.mean(y)\n",
    "        residuals = y - initial_prediction\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Create a weak learner (decision stump in this case)\n",
    "            tree = DecisionStump()\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Make predictions using the weak learner\n",
    "            predictions = tree.predict(X)\n",
    "\n",
    "            # Update residuals (negative gradient)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "\n",
    "            # Add the weak learner to the ensemble\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing up the predictions from each weak learner\n",
    "        predictions = np.sum(self.learning_rate * model.predict(X) for model in self.models)\n",
    "        return predictions\n",
    "\n",
    "# Weak learner (Decision Stump)\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.prediction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        best_mse = np.inf\n",
    "\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature_index] <= threshold\n",
    "                right_indices = X[:, feature_index] > threshold\n",
    "\n",
    "                left_y = y[left_indices]\n",
    "                right_y = y[right_indices]\n",
    "\n",
    "                mse = np.mean((left_y - np.mean(left_y)) ** 2) + np.mean((right_y - np.mean(right_y)) ** 2)\n",
    "\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    self.feature_index = feature_index\n",
    "                    self.threshold = threshold\n",
    "                    self.prediction = np.mean(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(X[:, self.feature_index] <= self.threshold, self.prediction, -self.prediction)\n",
    "\n",
    "# Instantiate and train Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X)\n",
    "\n",
    "# Calculate MSE and R-squared\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc04458-9de4-4f27-8d3e-a58b51749686",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88bf1be9-4198-4fea-9cfa-22dc0f420769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}\n",
      "Best Negative Mean Squared Error: -1.110053520074102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Grid Search with Cross-Validation\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Negative Mean Squared Error:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20cc453-5cf2-4c2b-86ea-4bc8e0924b89",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5364c-11d5-4f4b-b4c8-5b0058f618b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Gradient Boosting, a weak learner refers to a simple or basic model that performs slightly better than random guessing on a given problem. Specifically, it is a model that has limited predictive power by itself and doesn't need to be very complex. These weak learners are usually simple decision trees, often referred to as decision stumps, or sometimes even linear regression models.\n",
    "\n",
    "Characteristics of a Weak Learner:\n",
    "Limited Complexity:\n",
    "\n",
    "Weak learners are deliberately kept simple to avoid overfitting and reduce computational complexity.\n",
    "In the context of decision trees, these may be trees with shallow depth, often containing only a single split (decision stump).\n",
    "Slightly Better Than Random Guessing:\n",
    "\n",
    "While individually weak, these learners perform slightly better than random guessing on the training data.\n",
    "They might have a performance slightly above chance but are not powerful enough to capture the entire complexity of the problem.\n",
    "Focused on Specific Patterns:\n",
    "\n",
    "Weak learners focus on learning specific patterns or relationships within the data.\n",
    "They might excel in capturing certain parts of the data but lack the capacity to generalize well across the entire dataset.\n",
    "Contributions to Ensemble:\n",
    "\n",
    "In Gradient Boosting, weak learners are sequentially added to the ensemble, and each subsequent model tries to correct the errors made by the ensemble of existing models.\n",
    "By themselves, weak learners might not perform well, but their collective combination in boosting algorithms results in a strong and accurate model.\n",
    "Importance in Gradient Boosting:\n",
    "The strength of Gradient Boosting lies in its ability to sequentially combine multiple weak learners, each one focusing on the mistakes made by the ensemble of previous models.\n",
    "By iteratively adding weak models and adjusting their predictions to minimize the loss function, Gradient Boosting creates a robust ensemble model that outperforms individual weak learners.\n",
    "Conclusion:\n",
    "In Gradient Boosting, a weak learner is a simple, often low-complexity model that, while individually not very effective, contributes to the ensemble's overall predictive power when combined in a sequential manner. The effectiveness of Gradient Boosting comes from its ability to harness the collective knowledge of these weak learners to create a strong and accurate predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501fd936-1f4d-4b55-9669-40d0a399bff8",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cf51d-8c43-4083-beeb-34557c47385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to build a strong predictive model by sequentially combining multiple weak learners (typically decision trees) in a way that each subsequent learner corrects the errors made by the ensemble of existing models.\n",
    "\n",
    "Intuitive Steps of Gradient Boosting:\n",
    "Initiation with Simple Model:\n",
    "\n",
    "Gradient Boosting starts with an initial simple model that makes predictions based on a straightforward rule or the average target value (in regression tasks).\n",
    "This initial model provides a starting point for predictions but may not accurately capture the entire dataset's complexity.\n",
    "Sequential Improvement:\n",
    "\n",
    "Subsequent models are added sequentially, with each one focusing on the mistakes or residuals made by the ensemble of existing models.\n",
    "Each new model is trained to predict the residuals (errors) of the combined ensemble up to that point.\n",
    "Weighted Combination:\n",
    "\n",
    "The predictions of each model are combined through a weighted sum or average, where each model's prediction is weighted by a learning rate parameter.\n",
    "The learning rate determines the contribution of each weak learner to the overall ensemble.\n",
    "Gradient Descent Optimization:\n",
    "\n",
    "Gradient Boosting employs gradient descent optimization to minimize a loss function (e.g., mean squared error in regression) by adjusting the predictions of the ensemble.\n",
    "In each iteration, the algorithm calculates the gradient of the loss function with respect to the predictions, and subsequent models try to reduce this gradient.\n",
    "Focus on Residuals:\n",
    "\n",
    "Each new weak learner (decision tree) is built to capture the errors or residuals left by the combined ensemble of existing models.\n",
    "The goal is to progressively reduce the residuals by iteratively improving predictions in areas where the ensemble model performs poorly.\n",
    "Creation of Strong Model:\n",
    "\n",
    "By combining the collective knowledge of multiple weak learners, Gradient Boosting creates a strong and accurate predictive model that can capture complex patterns within the data.\n",
    "Overall Intuition:\n",
    "Gradient Boosting, through its iterative process, learns to fit the residuals or errors of the previous ensemble models, focusing on areas where the current model performs poorly. By repeatedly improving upon the mistakes made by the combined ensemble, it gradually builds a robust and accurate predictive model that outperforms individual weak learners.\n",
    "\n",
    "Conclusion:\n",
    "The intuition behind Gradient Boosting lies in its ability to learn from mistakes made by an ensemble of weak models, gradually refining predictions and creating a powerful ensemble model that captures complex relationships in the data, making it particularly effective in predictive modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a73bb-10cb-488d-b101-4104f5549251",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1eae8-b41f-447e-8da0-89575e42a397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e3409-9539-4354-8414-9218cf53a894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac775b9d-c8d3-4687-a2d3-db140d46c1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

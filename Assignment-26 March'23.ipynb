{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044bc37d-800d-4a4f-84e1-d92d5e05227d",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1eba82-0461-45ce-b9f6-ac22ca6143d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple Linear Regression:\n",
    "\n",
    "Simple Linear Regression is a statistical method that models the relationship between a single independent variable (\n",
    "�\n",
    "X) and a dependent variable (\n",
    "�\n",
    "Y). The goal is to find the best-fitting line, known as the regression line, that minimizes the sum of squared differences between the observed values of \n",
    "�\n",
    "Y and the values predicted by the linear model.\n",
    "\n",
    "The equation for simple linear regression is:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+ϵ\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "X is the independent variable.\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept (the value of \n",
    "�\n",
    "Y when \n",
    "�\n",
    "X is 0).\n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is the slope (the change in \n",
    "�\n",
    "Y for a one-unit change in \n",
    "�\n",
    "X).\n",
    "�\n",
    "ϵ is the error term, representing the unobserved factors that influence \n",
    "�\n",
    "Y but are not captured by the model.\n",
    "Example of Simple Linear Regression:\n",
    "\n",
    "Let's say we want to predict a student's final exam score (\n",
    "�\n",
    "Y) based on the number of hours they spend studying (\n",
    "�\n",
    "X). The simple linear regression model could be:\n",
    "\n",
    "Final Exam Score\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "×\n",
    "Hours of Study\n",
    "+\n",
    "�\n",
    "Final Exam Score=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " ×Hours of Study+ϵ\n",
    "\n",
    "Multiple Linear Regression:\n",
    "\n",
    "Multiple Linear Regression extends the concept of simple linear regression to model the relationship between multiple independent variables (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "n\n",
    "​\n",
    " ) and a dependent variable (\n",
    "�\n",
    "Y). The equation for multiple linear regression is:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "n\n",
    "​\n",
    "  are the independent variables.\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,...,β \n",
    "n\n",
    "​\n",
    "  are the coefficients representing the change in \n",
    "�\n",
    "Y for a one-unit change in each respective \n",
    "�\n",
    "X.\n",
    "�\n",
    "ϵ is the error term.\n",
    "Example of Multiple Linear Regression:\n",
    "\n",
    "Let's extend the previous example. Now, we want to predict a student's final exam score (\n",
    "�\n",
    "Y) based on the number of hours they spend studying (\n",
    "�\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    " ), the number of hours they sleep the night before the exam (\n",
    "�\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    " ), and their previous exam scores (\n",
    "�\n",
    "3\n",
    "X \n",
    "3\n",
    "​\n",
    " ). The multiple linear regression model could be:\n",
    "\n",
    "Final Exam Score\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "×\n",
    "Hours of Study\n",
    "+\n",
    "�\n",
    "2\n",
    "×\n",
    "Hours of Sleep\n",
    "+\n",
    "�\n",
    "3\n",
    "×\n",
    "Previous Exam Scores\n",
    "+\n",
    "�\n",
    "Final Exam Score=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " ×Hours of Study+β \n",
    "2\n",
    "​\n",
    " ×Hours of Sleep+β \n",
    "3\n",
    "​\n",
    " ×Previous Exam Scores+ϵ\n",
    "\n",
    "In summary, the main difference is that simple linear regression involves one independent variable, while multiple linear regression involves two or more independent variables to predict a dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2226c2-4aed-45e6-b8a1-1e3ebca53255",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23f63a-6873-47ba-8725-0e63d9cdb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear regression makes several assumptions about the data and the model. It's important to check these assumptions to ensure that the results and interpretations of the regression analysis are valid. Here are the key assumptions of linear regression:\n",
    "\n",
    "Linearity:\n",
    "\n",
    "Assumption: The relationship between the independent and dependent variables is linear.\n",
    "Check: Scatter plots of the variables against each other. A pattern that roughly follows a straight line suggests linearity.\n",
    "Independence of Residuals:\n",
    "\n",
    "Assumption: The residuals (the differences between observed and predicted values) are independent.\n",
    "Check: Durbin-Watson statistic, autocorrelation plots, or time series analysis for time-dependent data.\n",
    "Homoscedasticity:\n",
    "\n",
    "Assumption: The variance of the residuals is constant across all levels of the independent variables.\n",
    "Check: Residual plots against predicted values. Look for a constant spread of residuals.\n",
    "Normality of Residuals:\n",
    "\n",
    "Assumption: The residuals are normally distributed.\n",
    "Check: Normal probability plots, histograms of residuals, or statistical tests such as the Shapiro-Wilk test.\n",
    "No Perfect Multicollinearity:\n",
    "\n",
    "Assumption: The independent variables are not perfectly correlated with each other.\n",
    "Check: Variance Inflation Factor (VIF) for each independent variable.\n",
    "No Perfect Collinearity:\n",
    "\n",
    "Assumption: The independent variables are not a perfect linear function of each other.\n",
    "Check: Check for perfect collinearity by examining the correlation matrix or using other diagnostics.\n",
    "To check these assumptions, you can perform the following diagnostic checks:\n",
    "\n",
    "Residual Analysis: Examine the residuals (observed minus predicted values) to identify patterns or trends. Residual plots can help assess linearity, homoscedasticity, and independence of residuals.\n",
    "\n",
    "Normality Tests: Use statistical tests such as the Shapiro-Wilk test, Anderson-Darling test, or visualizations like Q-Q plots to assess the normality of residuals.\n",
    "\n",
    "VIF Calculation: Calculate the Variance Inflation Factor (VIF) for each independent variable to identify multicollinearity. High VIF values indicate potential multicollinearity.\n",
    "\n",
    "Durbin-Watson Statistic: Evaluate the Durbin-Watson statistic to check for independence of residuals. Values close to 2 suggest no autocorrelation.\n",
    "\n",
    "Residuals vs. Fitted Values Plot: Examine a plot of residuals against predicted values to check for homoscedasticity.\n",
    "\n",
    "It's important to note that violations of these assumptions do not necessarily invalidate the entire analysis, but they may affect the reliability and validity of the results. Depending on the severity of the violations, corrective actions such as data transformations or the use of robust regression techniques may be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c9277b-7b5f-4854-a9f6-de05f154a0d3",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293d42d-ceec-472f-9070-ce6f032b3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a linear regression model, the equation takes the form:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+ϵ\n",
    "\n",
    "Here, \n",
    "�\n",
    "Y is the dependent variable, \n",
    "�\n",
    "X is the independent variable, \n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept, \n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is the slope, and \n",
    "�\n",
    "ϵ is the error term.\n",
    "\n",
    "Intercept (\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ):\n",
    "\n",
    "Interpretation: The intercept represents the predicted value of the dependent variable when the independent variable is zero.\n",
    "Example: In the context of predicting the final exam score (\n",
    "�\n",
    "Y) based on the number of hours spent studying (\n",
    "�\n",
    "X), the intercept (\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ) would represent the predicted exam score when the student spends zero hours studying. However, this interpretation may not always be meaningful, especially if zero hours of study is not a plausible or relevant value.\n",
    "Slope (\n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ):\n",
    "\n",
    "Interpretation: The slope represents the change in the predicted value of the dependent variable for a one-unit change in the independent variable.\n",
    "Example: Using the same scenario, if \n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is, for instance, 5, it means that for every additional hour a student spends studying (\n",
    "�\n",
    "X), their predicted final exam score (\n",
    "�\n",
    "Y) is expected to increase by 5 points.\n",
    "Real-world Example:\n",
    "\n",
    "Let's consider a real-world scenario where we want to predict the price of a house (\n",
    "�\n",
    "Y) based on its size in square feet (\n",
    "�\n",
    "X). The linear regression model might look like:\n",
    "\n",
    "House Price\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "×\n",
    "Size of House\n",
    "+\n",
    "�\n",
    "House Price=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " ×Size of House+ϵ\n",
    "\n",
    "Intercept (\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ):\n",
    "\n",
    "Interpretation: The intercept represents the predicted price of the house when its size is zero square feet. However, this interpretation is not practically meaningful in this context, as a house cannot have zero size.\n",
    "Slope (\n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ):\n",
    "\n",
    "Interpretation: The slope represents the change in the predicted price of the house for a one-unit increase in its size (one additional square foot).\n",
    "Example: If \n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is, for instance, $200, it means that, on average, for every additional square foot of size, the predicted price of the house is expected to increase by $200.\n",
    "In summary, the intercept and slope in a linear regression model provide insights into the baseline prediction and the rate of change in the dependent variable for a one-unit change in the independent variable, respectively. The interpretation should always be made in the context of the specific variables involved in the regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e359353e-8cc9-4320-97dc-330e13f17788",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93a1c4-0ebf-4977-8fc9-c0407e4cebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent:\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm used for finding the minimum of a function. In the context of machine learning, it is commonly employed to minimize the cost or loss function associated with a model. The goal is to adjust the model parameters (weights) iteratively in the direction that reduces the cost function, eventually reaching a minimum or a point close to it.\n",
    "\n",
    "Here's a high-level overview of how gradient descent works:\n",
    "\n",
    "Initialize Parameters:\n",
    "\n",
    "Start with initial values for the model parameters.\n",
    "Compute the Gradient:\n",
    "\n",
    "Calculate the gradient of the cost function with respect to each parameter. The gradient indicates the direction of the steepest increase in the cost function.\n",
    "Update Parameters:\n",
    "\n",
    "Adjust the parameters in the opposite direction of the gradient to move towards the minimum. The size of the update is controlled by the learning rate, which determines the step size.\n",
    "Repeat:\n",
    "\n",
    "Repeat steps 2 and 3 until convergence, where the algorithm has found a minimum or a sufficiently low point in the cost function.\n",
    "Mathematical Representation:\n",
    "\n",
    "For a simple linear regression problem, the update rule for a parameter (\n",
    "�\n",
    "θ) using gradient descent is given by:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "θ=θ−α \n",
    "∂θ\n",
    "∂J\n",
    "​\n",
    " \n",
    "\n",
    "Here, \n",
    "�\n",
    "α is the learning rate, \n",
    "�\n",
    "J is the cost function, and \n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "∂θ\n",
    "∂J\n",
    "​\n",
    "  is the gradient of the cost function with respect to \n",
    "�\n",
    "θ.\n",
    "\n",
    "Types of Gradient Descent:\n",
    "\n",
    "Batch Gradient Descent:\n",
    "\n",
    "Uses the entire training dataset to compute the gradient of the cost function at each iteration.\n",
    "Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Updates the parameters using only one randomly chosen training sample at each iteration, making it faster but more noisy.\n",
    "Mini-Batch Gradient Descent:\n",
    "\n",
    "A compromise between batch and stochastic gradient descent. It updates the parameters using a small random subset (mini-batch) of the training data.\n",
    "Key Concepts:\n",
    "\n",
    "Learning Rate (\n",
    "�\n",
    "α):\n",
    "\n",
    "Determines the step size in the parameter space. A too small learning rate may result in slow convergence, while a too large one may cause overshooting.\n",
    "Convergence:\n",
    "\n",
    "The algorithm stops when the change in the cost function becomes negligible, indicating that it has reached a minimum or a stable point.\n",
    "Cost Function:\n",
    "\n",
    "The objective function that the algorithm aims to minimize. It measures the difference between the predicted and actual values.\n",
    "Usage in Machine Learning:\n",
    "\n",
    "Gradient descent is a fundamental optimization algorithm in machine learning. It is used to train various types of models, including linear regression, logistic regression, neural networks, and other supervised learning models. By minimizing the cost function, the algorithm finds optimal parameter values that result in accurate predictions on unseen data. Adjustments to the learning rate and other hyperparameters can be made to optimize the performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6699ee2-9f8d-4ed6-9694-bdf3c12e991d",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df747e-926e-4b72-ac64-26666558c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple Linear Regression Model:\n",
    "\n",
    "Multiple Linear Regression is an extension of simple linear regression that allows for the modeling of the relationship between a dependent variable (\n",
    "�\n",
    "Y) and two or more independent variables (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "n\n",
    "​\n",
    " ). The general form of the multiple linear regression equation is:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "\n",
    "Here:\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "n\n",
    "​\n",
    "  are the independent variables.\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept (the value of \n",
    "�\n",
    "Y when all \n",
    "�\n",
    "X values are zero).\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,...,β \n",
    "n\n",
    "​\n",
    "  are the coefficients, representing the change in \n",
    "�\n",
    "Y for a one-unit change in each respective \n",
    "�\n",
    "X.\n",
    "�\n",
    "ϵ is the error term, representing unobserved factors that influence \n",
    "�\n",
    "Y but are not captured by the model.\n",
    "Differences from Simple Linear Regression:\n",
    "\n",
    "Number of Independent Variables:\n",
    "\n",
    "Simple Linear Regression: Involves only one independent variable (\n",
    "�\n",
    "X).\n",
    "Multiple Linear Regression: Involves two or more independent variables (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "n\n",
    "​\n",
    " ).\n",
    "Equation Form:\n",
    "\n",
    "Simple Linear Regression: \n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+ϵ\n",
    "Multiple Linear Regression: \n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "Interpretation of Coefficients:\n",
    "\n",
    "Simple Linear Regression: \n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  represents the change in \n",
    "�\n",
    "Y for a one-unit change in \n",
    "�\n",
    "X.\n",
    "Multiple Linear Regression: \n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,...,β \n",
    "n\n",
    "​\n",
    "  represent the change in \n",
    "�\n",
    "Y for a one-unit change in each respective \n",
    "�\n",
    "X, holding other variables constant.\n",
    "Model Complexity:\n",
    "\n",
    "Simple Linear Regression: Simpler model with a single predictor.\n",
    "Multiple Linear Regression: More complex model with multiple predictors.\n",
    "Assumptions:\n",
    "\n",
    "Simple Linear Regression: Assumptions of linearity, independence, homoscedasticity, normality of residuals.\n",
    "Multiple Linear Regression: Extends the assumptions to include no perfect multicollinearity (no perfect linear relationship between independent variables).\n",
    "Example:\n",
    "\n",
    "Consider predicting the price of a house (\n",
    "�\n",
    "Y) based on both the size of the house (\n",
    "�\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    " ) and the number of bedrooms (\n",
    "�\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    " ). The multiple linear regression model would be:\n",
    "\n",
    "House Price\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "×\n",
    "Size of House\n",
    "+\n",
    "�\n",
    "2\n",
    "×\n",
    "Number of Bedrooms\n",
    "+\n",
    "�\n",
    "House Price=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " ×Size of House+β \n",
    "2\n",
    "​\n",
    " ×Number of Bedrooms+ϵ\n",
    "\n",
    "In this example, \n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  represents the change in house price for a one-unit increase in the size of the house, and \n",
    "�\n",
    "2\n",
    "β \n",
    "2\n",
    "​\n",
    "  represents the change in house price for a one-unit increase in the number of bedrooms, holding the other variable constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf41e1-6327-4e28-8bc7-fa981fe902b7",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02804629-2c57-447a-8ddd-de2003dab825",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity in Multiple Linear Regression:\n",
    "\n",
    "Multicollinearity is a phenomenon in multiple linear regression where two or more independent variables in the model are highly correlated, making it challenging to distinguish their individual effects on the dependent variable. It can cause problems in the estimation of the regression coefficients, leading to increased standard errors and reduced precision of the coefficient estimates. This makes it difficult to interpret the impact of each variable separately.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "\n",
    "Correlation Matrix:\n",
    "\n",
    "Examine the correlation matrix among the independent variables. High correlation coefficients (close to +1 or -1) suggest potential multicollinearity.\n",
    "Variance Inflation Factor (VIF):\n",
    "\n",
    "Calculate the Variance Inflation Factor for each independent variable. VIF quantifies how much the variance of an estimated regression coefficient increases if the predictors are correlated. A VIF greater than 5 or 10 is often considered indicative of multicollinearity.\n",
    "Tolerance:\n",
    "\n",
    "Tolerance is the reciprocal of the VIF. A low tolerance value (close to zero) suggests high multicollinearity.\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Remove Redundant Variables:\n",
    "\n",
    "If two or more variables are highly correlated, consider removing one of them from the model. Choose the variable that is less theoretically relevant or has less significance in the context of the analysis.\n",
    "Feature Engineering:\n",
    "\n",
    "Create new variables that combine or represent the information contained in the correlated variables. This may help reduce multicollinearity.\n",
    "Principal Component Analysis (PCA):\n",
    "\n",
    "Use PCA to transform the original variables into a new set of uncorrelated variables (principal components). However, interpreting the results becomes more challenging.\n",
    "Regularization Techniques:\n",
    "\n",
    "Techniques such as Ridge Regression or Lasso Regression introduce regularization penalties that can help mitigate the impact of multicollinearity on coefficient estimates.\n",
    "Increase Sample Size:\n",
    "\n",
    "Sometimes multicollinearity is more problematic in smaller datasets. Increasing the sample size can sometimes reduce the impact.\n",
    "Multicollinearity Diagnostic Plots:\n",
    "\n",
    "Plot residuals against each independent variable. Unusual patterns in these plots might indicate multicollinearity issues.\n",
    "Example:\n",
    "\n",
    "Consider a multiple linear regression model predicting a person's income based on their education level (in years) and the number of years of work experience. If education level and work experience are highly correlated (e.g., people with more education tend to have more work experience), it could lead to multicollinearity. Addressing this issue might involve choosing one variable over the other or creating a new variable that combines information from both (e.g., total years of education and work experience)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57519c9-2536-4def-8924-fab8db98db8f",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9334b4f-dc05-4870-82d5-114458295657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0ae3d-0e52-4a8d-b751-a366c6e44f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19a8f5-7680-45f3-a61b-c2a42d530416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

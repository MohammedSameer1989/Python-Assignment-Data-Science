{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633ecd76-e695-413c-9a2b-d4188a876ad3",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec719c-ab34-4658-8393-dc3c5027b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's denote:\n",
    "\n",
    "�\n",
    "A: the event that an employee uses the health insurance plan.\n",
    "�\n",
    "B: the event that an employee is a smoker.\n",
    "From the given information:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(A) = Probability that an employee uses the health insurance plan = 70% = 0.70\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(B∣A) = Probability that an employee is a smoker given that they use the health insurance plan = 40% = 0.40\n",
    "We want to find \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(B∣A), which represents the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "By Bayes' theorem:\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(B∣A)= \n",
    "P(A)\n",
    "P(A∩B)\n",
    "​\n",
    " \n",
    "\n",
    "Given that:\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "0.70\n",
    "P(A)=0.70\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "0.40\n",
    "P(B∣A)=0.40\n",
    "\n",
    "We need to find \n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "P(B∩A), the probability that an employee uses the health insurance plan and is a smoker.\n",
    "\n",
    "Using the formula:\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "×\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(B∩A)=P(A)×P(B∣A)\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "=\n",
    "0.70\n",
    "×\n",
    "0.40\n",
    "P(B∩A)=0.70×0.40\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "=\n",
    "0.28\n",
    "P(B∩A)=0.28\n",
    "\n",
    "Now, we can substitute the values into Bayes' theorem to find \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(B∣A):\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(B∣A)= \n",
    "P(A)\n",
    "P(B∩A)\n",
    "​\n",
    " \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "0.28\n",
    "0.70\n",
    "P(B∣A)= \n",
    "0.70\n",
    "0.28\n",
    "​\n",
    " \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "0.4\n",
    "P(B∣A)=0.4\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is \n",
    "0.4\n",
    "0.4 or \n",
    "40\n",
    "%\n",
    "40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b056a-8bcd-4a93-9b28-04585e9880ad",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d46ce-1f47-40ea-8ed3-60b6f3514f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "The difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in how they handle the probability distribution of features and their suitability for different types of data:\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Feature Representation: Bernoulli Naive Bayes is suitable for binary feature data, where features are binary variables representing presence or absence.\n",
    "Probability Distribution: It assumes a Bernoulli distribution for features, meaning each feature is a binary variable taking values 0 (absence) or 1 (presence).\n",
    "Usage: Commonly used in text classification problems where the occurrence (or absence) of words in documents is represented as binary features (e.g., using the Bag-of-Words model).\n",
    "Example: Spam detection in emails, where the presence or absence of specific words or features (e.g., \"free,\" \"offer\") is considered.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Feature Representation: Multinomial Naive Bayes is suitable for features representing counts or frequencies, typically used in text classification where features are counts of word occurrences.\n",
    "Probability Distribution: It assumes a multinomial distribution for features, where each feature represents the frequency count of occurrences of various categories.\n",
    "Usage: Often applied in text classification tasks where features are represented as word frequencies (e.g., using TF-IDF - Term Frequency-Inverse Document Frequency).\n",
    "Example: Document classification based on word frequencies, sentiment analysis using word counts in reviews.\n",
    "Key Differences:\n",
    "\n",
    "Type of Features: Bernoulli Naive Bayes works with binary features (presence/absence), whereas Multinomial Naive Bayes handles counts or frequencies of occurrences.\n",
    "\n",
    "Probability Distribution Assumption: Bernoulli Naive Bayes assumes a Bernoulli distribution for features, while Multinomial Naive Bayes assumes a multinomial distribution.\n",
    "\n",
    "Application: Bernoulli Naive Bayes is commonly used when dealing with binary data or presence/absence of features, such as in text classification for binary word presence. On the other hand, Multinomial Naive Bayes is applied when dealing with features represented as counts or frequencies, as commonly seen in text classification using word frequencies.\n",
    "\n",
    "The choice between Bernoulli and Multinomial Naive Bayes depends on the nature of the features in the dataset, especially when dealing with text or categorical data, and whether the features are binary or represent counts/frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e805cfc-a645-4d86-8f83-ca22a5083e55",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb08d1-132f-4703-88f0-749fb55747b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Bernoulli Naive Bayes, missing values need to be handled before applying the algorithm since it assumes binary features (representing presence or absence).\n",
    "\n",
    "There's no inherent mechanism within the Bernoulli Naive Bayes algorithm to handle missing values explicitly. Therefore, it's essential to preprocess the data to handle missing values appropriately before using Bernoulli Naive Bayes.\n",
    "\n",
    "Here are some common approaches to handle missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "Imputation with a Placeholder:\n",
    "\n",
    "Replace missing values with a specific placeholder value, considering it as another category. For binary features, you might use a separate value (e.g., 'missing') to indicate the absence of information.\n",
    "Deletion of Rows or Columns:\n",
    "\n",
    "If the missing values are in a small proportion compared to the dataset, you might opt to remove the rows or columns containing missing values. However, this might lead to loss of information if there are many missing values.\n",
    "Imputation using Simple Statistics:\n",
    "\n",
    "Fill missing values with statistical measures such as mean, median, or mode. However, for Bernoulli Naive Bayes, where features are binary, this method might not be directly applicable unless you interpret these imputed values as an additional category.\n",
    "Using Machine Learning Algorithms for Imputation:\n",
    "\n",
    "Employ more complex methods like k-Nearest Neighbors (KNN) or other machine learning algorithms to predict missing values based on other features. After imputation, the binary features can be re-encoded accordingly.\n",
    "Consider Imputation Based on Domain Knowledge:\n",
    "\n",
    "If available, consider domain-specific knowledge to impute missing values in a meaningful way that aligns with the nature of the data.\n",
    "Before applying Bernoulli Naive Bayes, it's crucial to carefully handle missing values to avoid bias or loss of information in the analysis. The choice of handling missing values depends on the dataset's characteristics, the proportion of missing data, and the specific requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00d975-bb1a-4cd0-9fb0-a73d0e82020a",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c65f0a-2db4-492b-9edb-8255ce17db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes the features follow a Gaussian (normal) distribution within each class. Despite its name, it is not limited to binary classification and can handle multiple classes.\n",
    "\n",
    "In the case of multi-class classification, Gaussian Naive Bayes calculates the likelihood of each class for a given set of input features based on the Gaussian probability distribution of each feature within each class. Then, it selects the class with the highest posterior probability as the predicted class for the input instance.\n",
    "\n",
    "The algorithm performs the following steps for multi-class classification:\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "Estimate the mean and variance of each feature for each class in the training dataset, assuming a Gaussian distribution.\n",
    "Calculate the prior probabilities for each class.\n",
    "Prediction Phase:\n",
    "\n",
    "Given a new set of input features, compute the likelihood of the features belonging to each class using the Gaussian probability density function.\n",
    "Multiply the likelihood by the prior probability of each class to obtain the posterior probabilities for each class.\n",
    "Select the class with the highest posterior probability as the predicted class for the input instance.\n",
    "Gaussian Naive Bayes is commonly used for multi-class classification problems, such as text categorization, medical diagnosis, and various other applications where the features are continuous and assumed to follow a Gaussian distribution within each class. It's important to note that while it works well in many scenarios, it makes the strong assumption of feature independence, which might not hold true in all real-world datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2454572-018c-44f7-b862-cbbd8d13eb8a",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: Create your assignment in Jupyter notebook and upload it to GitHub & share that github repository\n",
    "link through your dashboard. Make sure the repository is public.\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb6c54-3ac3-462f-9530-ea98c691f584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac2a94d-4238-4d39-88fa-7d32c7c5c544",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85379e5-a49c-4bc6-9509-64c5c6c41349",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-squared (coefficient of determination) is a statistical measure used to assess how well the independent variable(s) explain the variability of the dependent variable in a linear regression model. It represents the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "Mathematically, R-squared is calculated using the following formula:\n",
    "\n",
    "�\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "SSR\n",
    "SST\n",
    "R \n",
    "2\n",
    " =1− \n",
    "SST\n",
    "SSR\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "SSR (Sum of Squared Residuals) is the sum of the squared differences between the predicted values and the actual values of the dependent variable.\n",
    "SST (Total Sum of Squares) is the total variance in the dependent variable, calculated as the sum of the squared differences between the actual values of the dependent variable and its mean.\n",
    "R-squared values range from 0 to 1. Higher R-squared values indicate a better fit of the regression model to the data. A value of 1 indicates that all variability in the dependent variable is explained by the independent variable(s), while a value of 0 suggests that the independent variable(s) cannot explain the variability in the dependent variable.\n",
    "\n",
    "However, R-squared has limitations. It does not determine whether the regression model is causal or merely correlates variables. Additionally, R-squared can increase when adding more independent variables, even if those variables do not have real predictive power, leading to potential overfitting issues. Therefore, it's essential to assess R-squared alongside other metrics and perform thorough model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a16427-2975-4549-9c33-00c763ad4843",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23afcf1-cfdf-4d15-ab28-fdaebbec55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is a modified version of the regular R-squared (coefficient of determination) that adjusts for the number of predictors (independent variables) in a regression model. It addresses one of the limitations of the regular R-squared by penalizing the addition of irrelevant predictors to the model, providing a more reliable indication of the model's goodness-of-fit.\n",
    "\n",
    "The formula for adjusted R-squared is:\n",
    "\n",
    "Adjusted \n",
    "�\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "2\n",
    ")\n",
    "⋅\n",
    "(\n",
    "�\n",
    "−\n",
    "1\n",
    ")\n",
    "�\n",
    "−\n",
    "�\n",
    "−\n",
    "1\n",
    "Adjusted R \n",
    "2\n",
    " =1− \n",
    "n−k−1\n",
    "(1−R \n",
    "2\n",
    " )⋅(n−1)\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "2\n",
    "R \n",
    "2\n",
    "  is the regular R-squared.\n",
    "�\n",
    "n is the number of observations in the dataset.\n",
    "�\n",
    "k is the number of predictors (independent variables) in the model.\n",
    "The adjusted R-squared value typically differs from the regular R-squared when the number of predictors in the model changes. Unlike the regular R-squared, which generally increases with the addition of more predictors (even if they are irrelevant), adjusted R-squared accounts for the number of predictors and adjusts the value based on whether the added predictors improve the model's explanatory power beyond what would be expected by chance.\n",
    "\n",
    "Adjusted R-squared will increase only if the new predictor(s) added to the model improve the model's fit more than would be expected by chance. If adding a new predictor does not improve the model significantly, the adjusted R-squared may decrease, indicating that the additional variable(s) did not contribute meaningfully to explaining the variance in the dependent variable.\n",
    "\n",
    "In summary, adjusted R-squared serves as a more conservative measure than the regular R-squared, penalizing excessive use of predictors and providing a better assessment of the model's goodness-of-fit, particularly when dealing with multiple predictors in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ea8d2-9bae-4385-8c8c-cdfd99289bc6",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1bd72-fe31-4051-a38a-bc428d06419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is particularly useful in situations where you're dealing with multiple predictors (independent variables) in a regression model. Here are some scenarios where adjusted R-squared can be more appropriate:\n",
    "\n",
    "Comparing models with different numbers of predictors: When you're comparing models with different numbers of predictors, adjusted R-squared helps in assessing the goodness-of-fit while considering the penalty for adding unnecessary variables. It helps to determine whether adding more predictors improves the model significantly or if the increase in complexity outweighs the improvement in explanatory power.\n",
    "\n",
    "Avoiding overfitting: Adjusted R-squared penalizes the addition of irrelevant predictors, reducing the risk of overfitting. Overfitting occurs when a model fits too closely to the training data but fails to generalize well to new, unseen data. Adjusted R-squared is more conservative than the regular R-squared and helps in selecting a more parsimonious model that avoids overfitting.\n",
    "\n",
    "Model selection: When performing variable selection or feature engineering in regression analysis, adjusted R-squared assists in deciding which predictors to include in the model. It helps in identifying the most relevant variables that contribute meaningfully to explaining the variance in the dependent variable.\n",
    "\n",
    "Complex regression models: In complex regression models where multiple predictors are involved, adjusted R-squared provides a better evaluation of model fit by accounting for the number of predictors. This is crucial because including too many predictors, especially those that are not statistically significant or relevant, can lead to an overly optimistic assessment of the model's fit using regular R-squared.\n",
    "\n",
    "In essence, adjusted R-squared is more appropriate when dealing with regression models containing multiple predictors to ensure a more balanced evaluation of model fit and to guard against the inclusion of unnecessary variables that don't contribute meaningfully to explaining the dependent variable's variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e5adc-a3bd-4bae-b30d-37e68be3fef4",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9079dc7-d47c-4836-aebe-d32f597ca016",
   "metadata": {},
   "outputs": [],
   "source": [
    "In regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics to evaluate the performance of a regression model by measuring the differences between predicted values and actual values of the dependent variable.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "MSE is calculated by taking the average of the squared differences between predicted and actual values.\n",
    "Mathematically, MSE is calculated as follows:\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (Y \n",
    "i\n",
    "​\n",
    " − \n",
    "Y\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "Where:\n",
    "�\n",
    "�\n",
    "Y \n",
    "i\n",
    "​\n",
    "  represents the actual values.\n",
    "�\n",
    "^\n",
    "�\n",
    "Y\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    "  represents the predicted values.\n",
    "�\n",
    "n is the number of observations.\n",
    "Root Mean Squared Error (RMSE):\n",
    "\n",
    "RMSE is the square root of the MSE and provides the measure of the standard deviation of the residuals (the differences between predicted and actual values).\n",
    "Mathematically, RMSE is calculated as follows:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "RMSE= \n",
    "MSE\n",
    "​\n",
    " \n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "MAE is calculated by taking the average of the absolute differences between predicted and actual values.\n",
    "Mathematically, MAE is calculated as follows:\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    "∣\n",
    "MAE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣Y \n",
    "i\n",
    "​\n",
    " − \n",
    "Y\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    " ∣\n",
    "What do these metrics represent?\n",
    "\n",
    "MSE measures the average squared differences between predicted and actual values. Squaring these differences penalizes larger errors more than smaller errors.\n",
    "RMSE is the square root of MSE, providing a measure of the typical deviation of the predicted values from the actual values.\n",
    "MAE represents the average absolute differences between predicted and actual values. It doesn't penalize large errors as heavily as MSE since it uses absolute differences.\n",
    "Comparing these metrics:\n",
    "\n",
    "RMSE and MSE both put more weight on larger errors due to squaring, making them more sensitive to outliers compared to MAE.\n",
    "MAE is more robust to outliers since it doesn't involve squaring the errors.\n",
    "Lower values of these metrics indicate better model performance in terms of accuracy and closeness of predictions to actual values.\n",
    "When choosing among these metrics, consider the specific characteristics of the problem, such as sensitivity to outliers, as well as the interpretability of the chosen metric for your particular regression analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc75a8f-5c3e-4499-8150-39cd73cd2fd3",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340133f-3c7c-4ef8-99a6-616e4ab243f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! Each evaluation metric—RMSE, MSE, and MAE—has its own set of advantages and disadvantages when used in regression analysis:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "MSE (Mean Squared Error):\n",
    "\n",
    "Advantage: MSE gives higher weightage to larger errors due to squaring the differences. It helps penalize significant deviations more, making it suitable when larger errors need to be emphasized.\n",
    "Advantage: It's differentiable and well-suited for optimization algorithms in machine learning.\n",
    "RMSE (Root Mean Squared Error):\n",
    "\n",
    "Advantage: RMSE is directly interpretable in the same units as the dependent variable. This makes it easier to understand the scale of the error and relate it to the actual data.\n",
    "Advantage: It penalizes larger errors more, like MSE, but the square root operation helps alleviate the units' squaring effect, making it more aligned with the original scale.\n",
    "MAE (Mean Absolute Error):\n",
    "\n",
    "Advantage: MAE is more robust to outliers since it takes the absolute differences instead of squaring them. It provides a better representation of the average error magnitude without emphasizing extreme errors.\n",
    "Advantage: Its simplicity and ease of interpretation make it more intuitive and straightforward to explain to non-technical stakeholders.\n",
    "Disadvantages:\n",
    "\n",
    "MSE:\n",
    "\n",
    "Disadvantage: MSE can be heavily influenced by outliers due to squaring the errors. It makes the metric sensitive to extreme values, which might not represent the overall model performance accurately.\n",
    "RMSE:\n",
    "\n",
    "Disadvantage: RMSE inherits the sensitivity to outliers from MSE, as it's derived from MSE. Therefore, extreme values can significantly impact its value and interpretation.\n",
    "MAE:\n",
    "\n",
    "Disadvantage: MAE doesn't emphasize larger errors as much as MSE or RMSE. While this is beneficial in handling outliers, it might not be desirable if larger errors need to be given more weight in the evaluation.\n",
    "Disadvantage: Due to its absolute nature, MAE lacks the mathematical convenience of being differentiable, which might be a disadvantage in certain optimization algorithms used in machine learning models.\n",
    "Choosing the Right Metric:\n",
    "\n",
    "The choice of the evaluation metric depends on the specific context of the problem. If the focus is on emphasizing larger errors, MSE or RMSE might be suitable. However, if outliers are a concern and you want a more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892947ef-c363-452a-b074-7a0d50cb5fbc",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830edab2-28b7-4eb5-84ff-01d0d8f0f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression and other models to prevent overfitting and encourage simpler models by adding a penalty term to the regression equation.\n",
    "\n",
    "The key characteristic of Lasso regularization is its ability to perform feature selection by driving the coefficients of less important features toward zero, effectively eliminating them from the model. This is achieved by adding the absolute values of the coefficients as a penalty term to the cost function being optimized during model training.\n",
    "\n",
    "The Lasso regularization term is represented as:\n",
    "\n",
    "Lasso Regression Cost Function\n",
    "=\n",
    "MSE\n",
    "+\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "Lasso Regression Cost Function=MSE+λ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣β \n",
    "i\n",
    "​\n",
    " ∣\n",
    "\n",
    "Where:\n",
    "\n",
    "MSE represents the Mean Squared Error.\n",
    "�\n",
    "λ is the regularization parameter that controls the strength of the penalty term.\n",
    "�\n",
    "�\n",
    "β \n",
    "i\n",
    "​\n",
    "  represents the coefficients of the features.\n",
    "Differences between Lasso and Ridge regularization:\n",
    "\n",
    "Penalty Term:\n",
    "\n",
    "Lasso regularization adds the absolute values of the coefficients (\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "∣β \n",
    "i\n",
    "​\n",
    " ∣) as the penalty term.\n",
    "Ridge regularization adds the squared values of the coefficients (\n",
    "�\n",
    "�\n",
    "2\n",
    "β \n",
    "i\n",
    "2\n",
    "​\n",
    " ) as the penalty term.\n",
    "Feature Selection:\n",
    "\n",
    "Lasso tends to force the coefficients of less important features to become exactly zero, effectively performing feature selection.\n",
    "Ridge regression reduces the impact of less important features but rarely forces coefficients to become zero.\n",
    "Solution Space:\n",
    "\n",
    "The solution space for Lasso regularization tends to produce sparse models (with fewer non-zero coefficients), favoring sparsity.\n",
    "Ridge regularization generally maintains all features by reducing their impact without eliminating any, leading to less sparse models.\n",
    "When to use Lasso regularization:\n",
    "\n",
    "Feature selection: If the dataset contains a large number of features, and you suspect that many of them might be irrelevant or redundant, Lasso regularization can be more appropriate as it tends to eliminate less important features by setting their coefficients to zero.\n",
    "Sparse solutions: When looking for a simpler, more interpretable model with fewer variables, Lasso is preferable due to its ability to create sparse models by driving coefficients to zero.\n",
    "In summary, Lasso regularization is suitable when feature selection and sparsity are desirable, whereas Ridge regularization is more appropriate when reducing the impact of all features without eliminating any is the primary concern. The choice between Lasso and Ridge regularization often depends on the specific characteristics of the dataset and the goals of the modeling task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e9916-aca6-46d6-9f1b-7959b19d6e6a",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968de9a-3e50-492b-a92c-b59ed3150018",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a regularization term to the standard linear regression cost function. This additional term penalizes overly complex models by imposing constraints on the coefficients, discouraging them from reaching excessively large values. By doing so, regularization reduces the model's tendency to fit noise in the training data, promoting better generalization to unseen data.\n",
    "\n",
    "There are primarily two types of regularization techniques used in linear models: Lasso (L1 regularization) and Ridge (L2 regularization).\n",
    "\n",
    "Lasso (L1 regularization) adds the absolute values of the coefficients as a penalty term to the cost function.\n",
    "Ridge (L2 regularization) adds the squared values of the coefficients as a penalty term.\n",
    "Here's an example illustrating how regularized linear models prevent overfitting:\n",
    "\n",
    "Let's consider a scenario where you have a dataset with many features (e.g., predicting house prices based on various features like square footage, number of bedrooms, etc.), and you're using linear regression for modeling.\n",
    "\n",
    "Without regularization:\n",
    "\n",
    "With a standard linear regression model, you might end up with a situation where the model tries to fit the noise or idiosyncrasies present in the training data. This can lead to a model that captures the noise rather than the underlying patterns, resulting in overfitting.\n",
    "The model might assign substantial weights to many features, even those that are not genuinely informative for predicting house prices.\n",
    "With regularization:\n",
    "\n",
    "Applying Lasso or Ridge regularization in linear regression introduces a penalty term in the model's cost function.\n",
    "Lasso regularization tends to force the coefficients of less important features to become zero, effectively performing feature selection and creating a sparser model.\n",
    "Ridge regularization reduces the impact of less important features by shrinking their coefficients but rarely forces them to become zero.\n",
    "For instance, in Lasso regularization, features that are not essential for predicting house prices might have their coefficients reduced to zero, effectively removing them from the model. This prevents the model from overfitting by focusing only on the most relevant features.\n",
    "\n",
    "By controlling the complexity of the model through regularization, these techniques prevent overfitting by restraining the model's ability to fit noise and focusing on capturing the underlying patterns in the data. Regularization helps achieve better generalization performance on unseen data, improving the model's robustness and performance in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2bfaf1-0cc0-459c-a713-4abea71b300b",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268cdd1e-d2f5-41b3-9099-60b6adc2cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models, such as Lasso (L1 regularization) and Ridge (L2 regularization), offer several benefits in mitigating overfitting and improving model performance. However, they also have limitations that may make them less suitable or effective in certain situations for regression analysis:\n",
    "\n",
    "Loss of Information: Regularization techniques like Lasso can drive coefficients to exactly zero, effectively removing features from the model. While this feature selection can be advantageous by simplifying the model, it can also lead to a loss of potentially useful information. If some seemingly less important features contain valuable predictive information in specific cases, removing them entirely can reduce the model's performance.\n",
    "\n",
    "Bias-Variance Trade-off: Regularization methods primarily target the variance of the model. While they reduce variance and overfitting, they might introduce bias, leading to an underestimation or overestimation of relationships between features and the target variable. In scenarios where capturing fine details or subtle relationships is critical, excessive regularization might oversimplify the model and result in a biased estimation.\n",
    "\n",
    "Difficulty in Tuning Hyperparameters: Determining the optimal value for the regularization hyperparameter (lambda) can be challenging. Finding the right balance between bias and variance is crucial, but there's no universally optimal value for lambda. It often requires cross-validation or grid search methods, making the process computationally expensive and time-consuming, especially with large datasets or many features.\n",
    "\n",
    "Sensitive to Scaling: Regularization techniques are sensitive to feature scaling. If features have different scales, the regularization effect might disproportionately impact features with larger scales, influencing the importance attributed to different features.\n",
    "\n",
    "Not Suitable for Non-linear Relationships: Regularized linear models assume linear relationships between predictors and the target variable. If the underlying relationships are highly non-linear, these models may not capture the complexities present in the data, leading to suboptimal performance.\n",
    "\n",
    "Interpretability Concerns: While regularization helps in feature selection and simplification, the resulting models might sacrifice interpretability. For certain applications where understanding the individual impact of features is essential, highly regularized models might not be the best choice.\n",
    "\n",
    "Limited for High-Dimensional Data: Regularized models may struggle or become less effective when dealing with extremely high-dimensional data, especially when the number of features is much larger than the number of observations. In such cases, regularization might not effectively handle the \"curse of dimensionality.\"\n",
    "\n",
    "In summary, while regularized linear models offer valuable solutions to combat overfitting and improve generalization, they might not always be the best choice for regression analysis, especially in scenarios where feature interpretability, non-linearity, or balancing bias-variance trade-offs are critical. The choice of models should consider the specific characteristics of the dataset and the objectives of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec28ea2-314f-4957-9a0b-da361f40772c",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6e77c-2771-4beb-b557-e4f2f984fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparing Model A and Model B based solely on their evaluation metrics (RMSE and MAE), the decision of choosing the better performer depends on the specific context and the importance of different aspects of model performance.\n",
    "\n",
    "RMSE of Model A: 10\n",
    "\n",
    "RMSE (Root Mean Squared Error) represents the standard deviation of the residuals and places more weight on larger errors due to squaring. In this case, an RMSE of 10 indicates that, on average, the predictions deviate from the actual values by 10 units.\n",
    "MAE of Model B: 8\n",
    "\n",
    "MAE (Mean Absolute Error) represents the average absolute differences between predicted and actual values. An MAE of 8 indicates that, on average, the absolute difference between predictions and actual values is 8 units.\n",
    "Considering which model might be a better performer:\n",
    "\n",
    "Lower Error: Model B (with an MAE of 8) generally suggests a lower average error compared to Model A (with an RMSE of 10). Hence, based solely on these metrics, Model B appears to have a better performance in terms of the average absolute deviation from the true values.\n",
    "\n",
    "Limitations of Metrics:\n",
    "\n",
    "Sensitivity to Outliers: Both RMSE and MAE can be sensitive to outliers. RMSE tends to heavily penalize larger errors due to squaring, whereas MAE treats all errors equally.\n",
    "Scale Interpretation: RMSE is on the same scale as the dependent variable, while MAE is also on the same scale but represents the average absolute error.\n",
    "Decision Context: The choice between RMSE and MAE should also consider the specific context of the problem and the consequences of different types of errors. If larger errors have more significant implications, RMSE might be more appropriate despite being higher in value.\n",
    "Therefore, the selection of the \"better\" model should not solely rely on a single metric but should consider the trade-offs and context of the problem. A lower error in MAE (Model B) might indicate better average performance, but the decision should also involve assessing other factors like the nature of errors, sensitivity to outliers, and the overall impact on the specific application or business goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29126a-2926-4694-ac84-e941db9a526a",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3b3d4-73b0-4aa3-96f9-9d96372c48f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239bd963-a9e2-4a18-b0d9-1cfb24db8200",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a3510-606e-41dc-b39b-9b69f0715e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is an ensemble learning technique in machine learning where a sequence of weak learners (models that are only slightly better than random guessing) is combined to create a strong learner. The primary goal of boosting is to improve the predictive performance by focusing on instances that previous models have failed to classify correctly. It sequentially builds multiple weak models, each one trying to correct the errors of its predecessor.\n",
    "\n",
    "Key Characteristics of Boosting:\n",
    "Sequential Learning:\n",
    "\n",
    "Boosting algorithms train a series of models iteratively, where each subsequent model aims to correct the errors made by the previous ones.\n",
    "Weighted Training Data:\n",
    "\n",
    "Each instance in the dataset is assigned a weight, and misclassified instances are given higher weights to be focused on by the subsequent models.\n",
    "Combining Weak Learners:\n",
    "\n",
    "Weak learners, often simple models (e.g., decision trees), are combined to create a strong learner with improved predictive performance.\n",
    "Adaptive Learning:\n",
    "\n",
    "The algorithm adaptively changes the weights of misclassified instances during each iteration, directing the subsequent models to focus more on difficult-to-classify instances.\n",
    "Types of Boosting Algorithms:\n",
    "AdaBoost (Adaptive Boosting):\n",
    "\n",
    "One of the earliest and widely used boosting algorithms.\n",
    "Assigns higher weights to misclassified instances and combines multiple weak classifiers to form a strong model.\n",
    "Gradient Boosting:\n",
    "\n",
    "Builds models in a stage-wise fashion, where each new model corrects the residuals (errors) of the previous models.\n",
    "Examples include XGBoost, LightGBM, and CatBoost.\n",
    "Extreme Gradient Boosting (XGBoost):\n",
    "\n",
    "An optimized version of gradient boosting that includes regularization, handles missing values, and has faster execution.\n",
    "LightGBM and CatBoost:\n",
    "\n",
    "Other boosting algorithms designed for efficiency, handling categorical variables, and providing better accuracy.\n",
    "Advantages of Boosting:\n",
    "Improved Predictive Performance: Boosting often produces higher accuracy compared to individual weak models.\n",
    "Handles Complex Relationships: Effective in capturing complex patterns in data and nonlinear relationships.\n",
    "Robustness: Less prone to overfitting due to adaptive learning and focusing on misclassified instances.\n",
    "Use Cases:\n",
    "Boosting techniques are used in various domains such as finance (credit scoring), healthcare (disease prediction), and natural language processing (text classification), among others.\n",
    "Conclusion:\n",
    "Boosting is a powerful ensemble learning technique that sequentially combines weak learners to create a strong model, focusing on difficult instances to improve overall predictive accuracy. It is widely used due to its ability to handle complex relationships and achieve high accuracy in various machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c3e0a-b76a-4e80-8aba-c7966baea05e",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7c6fb-1c62-4e34-82f6-f5b907747a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting techniques offer several advantages in machine learning, but they also come with certain limitations. Here's an overview of their advantages and limitations:\n",
    "\n",
    "Advantages of Boosting Techniques:\n",
    "Improved Predictive Performance:\n",
    "\n",
    "Boosting methods often achieve higher accuracy compared to individual weak models by combining them into a strong learner. They are effective in reducing bias and variance, leading to better generalization.\n",
    "Handles Complex Relationships:\n",
    "\n",
    "Boosting algorithms can capture complex patterns and nonlinear relationships in data. They are capable of learning intricate decision boundaries and interactions among features.\n",
    "Robustness to Overfitting:\n",
    "\n",
    "Boosting algorithms are less prone to overfitting due to their sequential learning nature, which focuses on instances that are hard to classify. They adaptively adjust weights to improve the model's performance.\n",
    "Feature Importance Estimation:\n",
    "\n",
    "Some boosting algorithms provide insights into feature importance, helping in feature selection and understanding which features contribute most to predictions.\n",
    "Versatility and Adaptability:\n",
    "\n",
    "Boosting algorithms can be used with various base models (weak learners) and handle different types of data, including structured, unstructured, and categorical data.\n",
    "Limitations of Boosting Techniques:\n",
    "Sensitivity to Noisy Data:\n",
    "\n",
    "Boosting methods can be sensitive to noisy data or outliers. They might give undue importance to misclassified instances, leading to suboptimal performance.\n",
    "Computational Complexity:\n",
    "\n",
    "Training boosting models can be computationally expensive and time-consuming, especially for large datasets and complex models. Some variants, such as gradient boosting, can be resource-intensive.\n",
    "Potential Overfitting with Too Many Iterations:\n",
    "\n",
    "If not properly tuned, boosting models can overfit, especially when there are too many iterations or weak models are too complex. This might degrade performance on unseen data.\n",
    "Parameter Sensitivity:\n",
    "\n",
    "Boosting models have hyperparameters that need careful tuning. Improper hyperparameter settings might affect the model's performance significantly.\n",
    "Less Interpretability:\n",
    "\n",
    "Compared to simpler models, boosting models are less interpretable due to their complexity and the sequential nature of learning.\n",
    "Conclusion:\n",
    "Boosting techniques offer several advantages, such as improved accuracy, handling complex relationships, and robustness to overfitting. However, they might be sensitive to noisy data, computationally expensive, and require careful parameter tuning. Understanding these advantages and limitations helps practitioners choose and utilize boosting techniques effectively based on the specific requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bef08c-f266-4482-a866-7ea54fe66927",
   "metadata": {},
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb40cd-e8ed-459b-aa00-28951085b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is an ensemble learning technique that combines multiple weak learners (models that perform slightly better than random guessing) to create a strong learner. It works sequentially by iteratively improving upon the mistakes made by the previous models, ultimately producing a more accurate and robust predictive model. The fundamental idea behind boosting can be explained in several steps:\n",
    "\n",
    "Basic Working of Boosting:\n",
    "Initialization:\n",
    "\n",
    "Boosting begins by training a base (weak) learner on the original dataset. This base learner can be any simple model capable of learning from the data, often a decision tree with limited depth (e.g., a stump).\n",
    "Weighted Training Data:\n",
    "\n",
    "Each instance in the dataset is assigned an equal weight initially.\n",
    "During subsequent iterations, higher weights are assigned to misclassified instances or those where the model made errors.\n",
    "Sequential Learning:\n",
    "\n",
    "Iteratively, new weak learners are trained on the modified dataset (with adjusted weights) to focus on the previously misclassified instances.\n",
    "Each new model aims to correct the errors made by the combined predictions of the previous models.\n",
    "Weighted Voting or Combining Predictions:\n",
    "\n",
    "Predictions from all weak learners are combined using weighted voting (or weighted averaging) to produce the final ensemble prediction.\n",
    "Adaptive Learning:\n",
    "\n",
    "The learning process adapts by giving more weight to instances that were incorrectly classified by earlier models. This process iterates until a predefined stopping criterion is met (e.g., a maximum number of iterations reached, no further improvement, etc.).\n",
    "Final Strong Learner:\n",
    "\n",
    "The final model is a weighted combination of all weak learners, where their collective predictions contribute to producing a more accurate and robust model.\n",
    "Key Points:\n",
    "Sequential Improvement: Each new model in the sequence focuses on the mistakes of the previous models, gradually reducing the overall error.\n",
    "\n",
    "Emphasis on Misclassified Instances: Boosting gives more attention to difficult-to-classify instances by adapting the training data weights, allowing the model to learn from its mistakes and improve.\n",
    "\n",
    "Aggregation of Weak Learners: Multiple weak learners are combined to create a strong ensemble model that outperforms individual models.\n",
    "\n",
    "Examples of Boosting Algorithms:\n",
    "AdaBoost (Adaptive Boosting)\n",
    "Gradient Boosting (including XGBoost, LightGBM, CatBoost)\n",
    "Stochastic Gradient Boosting (SGB)\n",
    "Conclusion:\n",
    "Boosting is an iterative ensemble learning technique that builds a strong model by sequentially training multiple weak learners and focusing on difficult instances. It aims to improve predictive accuracy by combining the collective knowledge of multiple models and iteratively refining predictions to create a robust ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a1b7e-3ade-4193-9ce3-ead9966a5959",
   "metadata": {},
   "source": [
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee3f43-9c97-476f-b085-b0e614719f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting algorithms are diverse and have evolved over time, leading to several variations that differ in their methodologies and techniques. Here are some of the prominent types of boosting algorithms:\n",
    "\n",
    "1. AdaBoost (Adaptive Boosting):\n",
    "AdaBoost is one of the earliest and widely used boosting algorithms.\n",
    "Focuses on misclassified instances by assigning higher weights to them, allowing subsequent weak learners to learn from the mistakes of previous models.\n",
    "It combines weak learners sequentially, adjusting the weights of instances to prioritize difficult-to-classify samples.\n",
    "2. Gradient Boosting:\n",
    "Gradient Boosting builds models in a stage-wise fashion by sequentially fitting new models to the residual errors (the difference between predictions and actual targets) made by the previous models.\n",
    "It minimizes a loss function by adding new models that correct the residuals of the existing model.\n",
    "Examples include XGBoost, LightGBM, and CatBoost, which are optimized and enhanced versions of gradient boosting algorithms with additional features for performance and accuracy.\n",
    "3. Stochastic Gradient Boosting (SGB):\n",
    "Stochastic Gradient Boosting extends gradient boosting by introducing randomness in the sample selection process during model training.\n",
    "It uses subsets of data (randomly sampled instances) and features (randomly sampled features) to improve diversity and reduce overfitting.\n",
    "Helps in speeding up training and improving robustness.\n",
    "4. LPBoost (Linear Programming Boosting):\n",
    "A boosting algorithm that frames boosting as a linear programming problem.\n",
    "It aims to minimize the exponential loss by constructing a linear combination of weak learners subject to linear constraints.\n",
    "5. LogitBoost:\n",
    "Similar to AdaBoost but focuses on minimizing the logistic loss function.\n",
    "It adapts the weights of misclassified instances using a Newton-Raphson optimization process.\n",
    "6. BrownBoost:\n",
    "Uses a modification of the AdaBoost algorithm by incorporating a Brownian motion model.\n",
    "It attempts to minimize classification error and maximize the margin simultaneously.\n",
    "7. LPAdaboost:\n",
    "A boosting algorithm that uses linear programming techniques to solve the optimization problem in boosting.\n",
    "Conclusion:\n",
    "Each type of boosting algorithm has its characteristics and approaches to iteratively improve the predictive performance by combining weak learners. The choice of the boosting algorithm depends on the dataset characteristics, problem requirements, and considerations regarding computational efficiency, interpretability, and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd342af-fd4b-4cf3-9300-c70bbf12f68a",
   "metadata": {},
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935de27-1caa-4ea8-8922-f42c511b923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting algorithms, including AdaBoost, Gradient Boosting (e.g., XGBoost, LightGBM), and others, have specific parameters that control the learning process, model complexity, and optimization. Some common parameters found in boosting algorithms are:\n",
    "\n",
    "1. Number of Estimators (n_estimators):\n",
    "Determines the number of weak learners (base models) to be sequentially trained during boosting.\n",
    "Increasing the number of estimators may lead to improved performance but can also increase computational complexity.\n",
    "2. Learning Rate (or Shrinkage):\n",
    "Controls the contribution of each model to the ensemble in gradient boosting.\n",
    "A smaller learning rate requires more models to achieve convergence but might result in better generalization.\n",
    "3. Max Depth (max_depth):\n",
    "Specifies the maximum depth allowed for each individual weak learner (e.g., decision trees) in the ensemble.\n",
    "Helps control the complexity of the weak models and prevents overfitting.\n",
    "4. Subsample (subsample or subsample_ratio):\n",
    "Determines the fraction of samples used for training each weak learner.\n",
    "Introduces randomness by training on a subset of data, reducing overfitting and speeding up training.\n",
    "5. Loss Function:\n",
    "Specifies the function to be minimized during the boosting process (e.g., exponential loss in AdaBoost, various loss functions in gradient boosting such as squared loss, logistic loss, etc.).\n",
    "6. Regularization Parameters:\n",
    "Specific to some boosting implementations, these parameters (like gamma, alpha, lambda) control regularization to prevent overfitting and improve model robustness.\n",
    "7. Feature Parameters:\n",
    "Parameters controlling feature selection, importance calculation, or handling of categorical variables (e.g., feature_importances_ in feature selection or handling categorical features using specific encodings).\n",
    "8. Early Stopping:\n",
    "A technique to prevent overfitting by stopping training when a certain metric (e.g., validation error) stops improving.\n",
    "Parameters define conditions like early_stopping_rounds and eval_metric.\n",
    "Conclusion:\n",
    "The choice and tuning of these parameters significantly impact the performance, speed, and generalization ability of boosting algorithms. Understanding these parameters and their effects is crucial for optimizing and fine-tuning boosting models for specific tasks and datasets. Each boosting implementation might have additional parameters specific to its functionality and optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f121aa-2baf-4870-bc48-57db5bf43d00",
   "metadata": {},
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbc25c-76dd-4a0a-b599-bf1dc7a46a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting algorithms combine weak learners sequentially to create a strong learner by emphasizing and focusing on the mistakes made by the previous models. The process involves iteratively training multiple weak models and adjusting their predictions to collectively produce a more accurate and robust model. The general mechanism of how boosting algorithms combine weak learners can be explained as follows:\n",
    "\n",
    "1. Sequential Learning:\n",
    "Boosting algorithms build a series of weak learners, typically simple models (e.g., decision trees, shallow models), in a sequential manner.\n",
    "Each new model is trained to correct the errors or misclassifications made by the ensemble of previously trained models.\n",
    "2. Weighted Data:\n",
    "Initially, each instance in the dataset has equal weights assigned to it.\n",
    "During each iteration, weights are adjusted, assigning higher weights to the misclassified instances or those instances that the previous models found difficult to classify correctly.\n",
    "3. Emphasis on Misclassified Instances:\n",
    "Emphasis is placed on instances that were incorrectly classified by the ensemble of weak learners. The subsequent models aim to handle these instances more effectively.\n",
    "4. Aggregate Predictions:\n",
    "Each weak learner contributes its prediction to the ensemble, and these predictions are combined using weighted voting (in classification) or weighted averaging (in regression) to generate the final prediction of the ensemble model.\n",
    "5. Adaptive Learning:\n",
    "The learning process adapts and focuses more on challenging instances by adjusting the weights and directing subsequent models to learn from the errors made by the ensemble of weaker models.\n",
    "6. Constructing a Strong Learner:\n",
    "By iteratively building multiple weak models that focus on the difficult-to-classify instances, boosting algorithms create a strong learner that leverages the collective knowledge of the weak models to improve predictive accuracy.\n",
    "Conclusion:\n",
    "Boosting algorithms build a strong ensemble model by training a sequence of weak learners, each one specializing in correcting the errors of the ensemble from previous iterations. This sequential learning process, combined with adaptive weighting of instances and aggregation of predictions, leads to the creation of a more accurate and robust predictive model compared to individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9abdd-b20f-4b27-9126-5905316913bf",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce560ac-c239-482e-977e-6c7409cffdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost (Adaptive Boosting) is a boosting algorithm that combines multiple weak learners (typically decision trees with limited depth) to create a strong ensemble model. It sequentially trains a series of weak learners and focuses on instances that are difficult to classify correctly. The primary objective of AdaBoost is to improve the predictive accuracy by adjusting the weights of misclassified instances during each iteration.\n",
    "\n",
    "Working of AdaBoost Algorithm:\n",
    "Initialization:\n",
    "\n",
    "Initially, each instance in the dataset is assigned an equal weight.\n",
    "Training Weak Learners:\n",
    "\n",
    "AdaBoost starts by training a weak learner (e.g., decision stump - a single-level decision tree) on the original dataset.\n",
    "The weak learner aims to minimize the weighted error, focusing on the misclassified instances in the training set.\n",
    "Instance Weight Adjustment:\n",
    "\n",
    "After the first iteration, the algorithm adjusts the weights of misclassified instances, increasing the weights of those instances that were classified incorrectly by the previous model.\n",
    "The higher weights indicate that these instances are more challenging and require more attention in subsequent iterations.\n",
    "Sequential Learning:\n",
    "\n",
    "AdaBoost continues by sequentially training additional weak learners, with each new learner aiming to correct the mistakes of the previous models.\n",
    "Each new model is trained on a modified dataset where instances are weighted based on their previous classification accuracy.\n",
    "Weighted Voting:\n",
    "\n",
    "Predictions from each weak learner are combined using weighted voting.\n",
    "More accurate models have higher weights in the final prediction.\n",
    "Final Strong Learner:\n",
    "\n",
    "The final prediction is generated by aggregating the weighted votes or predictions from all weak learners.\n",
    "The combined model is considered a strong learner that improves upon the weaknesses of individual weak models.\n",
    "Key Aspects of AdaBoost:\n",
    "Adaptive Weighting: Adjusts instance weights to focus on difficult-to-classify instances in subsequent iterations.\n",
    "Sequential Learning: Builds a series of weak learners, each one correcting the errors made by the ensemble of previous models.\n",
    "Weighted Voting: Combines predictions from weak learners using weighted voting to produce the final ensemble prediction.\n",
    "Conclusion:\n",
    "AdaBoost is an effective ensemble learning algorithm that creates a strong learner by iteratively training weak models and focusing on misclassified instances. By emphasizing difficult instances during training, AdaBoost aims to progressively improve predictive accuracy and create a robust ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7acf4-d23d-413e-9d25-85f8022aaded",
   "metadata": {},
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a42a36-3d86-4777-b7f8-e04f294e3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the AdaBoost (Adaptive Boosting) algorithm, the primary objective is to minimize the exponential loss function. The exponential loss function is utilized to evaluate the performance of weak learners and assign weights to instances based on their classification accuracy.\n",
    "\n",
    "Exponential Loss Function:\n",
    "The exponential loss function \n",
    "�\n",
    "L is defined as:\n",
    "\n",
    "�\n",
    "=\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "exp\n",
    "⁡\n",
    "(\n",
    "−\n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    ")\n",
    "L=∑ \n",
    "i=1\n",
    "N\n",
    "​\n",
    " exp(−y \n",
    "i\n",
    "​\n",
    " ⋅f(x \n",
    "i\n",
    "​\n",
    " ))\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "N is the number of instances in the dataset.\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "(x \n",
    "i\n",
    "​\n",
    " ,y \n",
    "i\n",
    "​\n",
    " ) represents the \n",
    "�\n",
    "i-th instance and its corresponding true label.\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "f(x \n",
    "i\n",
    "​\n",
    " ) denotes the prediction made by the weak learner for instance \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    " .\n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the true label of instance \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    " , where \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is either -1 or 1 (for binary classification).\n",
    "Explanation:\n",
    "In AdaBoost, the exponential loss function quantifies the misclassification errors made by the weak learner.\n",
    "It assigns higher penalties to misclassified instances by exponentially increasing the loss for instances that are incorrectly classified (where \n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "y \n",
    "i\n",
    "​\n",
    " ⋅f(x \n",
    "i\n",
    "​\n",
    " ) is negative).\n",
    "Conversely, correctly classified instances (where \n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "y \n",
    "i\n",
    "​\n",
    " ⋅f(x \n",
    "i\n",
    "​\n",
    " ) is positive) contribute less to the overall loss.\n",
    "Weight Update:\n",
    "After each weak learner is trained and evaluated on the exponential loss, the instance weights are adjusted based on the performance of the weak learner.\n",
    "Higher weights are assigned to misclassified instances, directing the subsequent weak learners to focus more on these difficult-to-classify instances in the next iteration.\n",
    "Conclusion:\n",
    "AdaBoost employs the exponential loss function to assess the performance of weak learners and adaptively update instance weights to emphasize misclassified instances. By minimizing the exponential loss, AdaBoost aims to iteratively improve the model's performance and create a strong ensemble model that excels in predicting difficult instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96985f9c-3922-4f99-8808-49e70a567790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ec5ee-9a7f-4aa5-891f-3bd51cb5bcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01018b-457a-4b24-b469-12c15a587799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f84079-a4ce-4d61-99f6-374cc87d69f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89da78ae-c861-45ec-8198-8b3aee744e22",
   "metadata": {},
   "source": [
    "Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in\n",
    "predicting the quality of wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be7128-39c9-4eca-b5ad-1081523b079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "The \"wine quality\" dataset typically refers to a dataset that contains information about various attributes of wines and their corresponding quality ratings. While specific datasets may vary, common features found in wine quality datasets include both physicochemical properties and sensory data. Here are some key features commonly found in such datasets and their importance in predicting the quality of wine:\n",
    "\n",
    "Fixed Acidity:\n",
    "\n",
    "Importance: Fixed acidity is a measure of the non-volatile acids present in the wine. It contributes to the overall taste and structure of the wine. Wines with an appropriate level of acidity are often considered more balanced.\n",
    "Volatile Acidity:\n",
    "\n",
    "Importance: Volatile acidity represents the presence of volatile acids, primarily acetic acid, which can contribute to undesirable vinegar-like flavors. Maintaining an appropriate level of volatile acidity is crucial for wine quality.\n",
    "Citric Acid:\n",
    "\n",
    "Importance: Citric acid is a weak organic acid found in some wines. It can contribute to the overall freshness and flavor of the wine, providing a crisp and citrusy character.\n",
    "Residual Sugar:\n",
    "\n",
    "Importance: Residual sugar is the amount of sugar remaining in the wine after fermentation. It influences the sweetness level of the wine. Balancing residual sugar is essential for achieving the desired sweetness level based on the wine style.\n",
    "Chlorides:\n",
    "\n",
    "Importance: Chlorides, primarily in the form of sodium chloride, can impact the taste and mouthfeel of wine. An appropriate level of chlorides contributes to the wine's overall balance.\n",
    "Free Sulfur Dioxide:\n",
    "\n",
    "Importance: Free sulfur dioxide is added to wines as a preservative and antioxidant. It helps prevent oxidation and microbial spoilage. Maintaining an optimal level of free sulfur dioxide is critical for wine stability.\n",
    "Total Sulfur Dioxide:\n",
    "\n",
    "Importance: Total sulfur dioxide includes both free and bound sulfur dioxide. It is another measure of the wine's stability and ability to resist spoilage.\n",
    "Density:\n",
    "\n",
    "Importance: Density is a measure of the wine's mass per unit volume. It can provide information about the wine's body and texture, influencing the overall mouthfeel.\n",
    "pH:\n",
    "\n",
    "Importance: pH is a measure of the wine's acidity level. It influences the taste, color, and stability of the wine. Wines with an appropriate pH are often more balanced and less susceptible to microbial spoilage.\n",
    "Sulphates:\n",
    "\n",
    "Importance: Sulphates, or sulfites, are additives used in winemaking to prevent oxidation and microbial growth. They contribute to the wine's stability and longevity.\n",
    "Alcohol:\n",
    "Importance: The alcohol content of the wine affects its body, mouthfeel, and overall perception. Wines with a well-balanced alcohol level are often more harmonious.\n",
    "These features collectively provide a comprehensive profile of the wine, capturing both chemical and sensory aspects. Analyzing and understanding these features can help winemakers, researchers, and enthusiasts predict and improve the quality of wines by making informed decisions during the winemaking process. Machine learning models can also be trained on such data to predict wine quality based on these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7b362-147d-4bb4-8687-df6808d66d72",
   "metadata": {},
   "source": [
    "Q2. How did you handle missing data in the wine quality data set during the feature engineering process?\n",
    "Discuss the advantages and disadvantages of different imputation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d18add-272d-45c5-b7b8-2f98f52c0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling missing data is a crucial step in the feature engineering process, as it can significantly impact the performance and validity of machine learning models. Common techniques for handling missing data include:\n",
    "\n",
    "Dropping Missing Values:\n",
    "\n",
    "Advantages: Simple and straightforward. It eliminates instances with missing values, ensuring that all remaining data is complete.\n",
    "Disadvantages: May result in a significant loss of data, especially if missing values are prevalent. It may introduce bias if missingness is not random.\n",
    "Mean/Median/Mode Imputation:\n",
    "\n",
    "Advantages: Simple and quick. It replaces missing values with the mean, median, or mode of the observed values for that feature.\n",
    "Disadvantages: Ignores the potential relationship between missing values and other variables. May not be suitable for variables with skewed distributions.\n",
    "Forward Fill/Backward Fill:\n",
    "\n",
    "Advantages: Suitable for time-series data. It fills missing values with the most recent non-missing value (forward fill) or the next non-missing value (backward fill).\n",
    "Disadvantages: May not be appropriate for non-time-series data. The assumption is that the data is ordered in a meaningful way.\n",
    "Interpolation Methods:\n",
    "\n",
    "Advantages: Utilizes relationships between variables to estimate missing values. Various methods, such as linear interpolation or spline interpolation, can be applied.\n",
    "Disadvantages: Sensitive to the assumptions about the relationships between variables. May not perform well if relationships are complex or nonlinear.\n",
    "Multiple Imputation:\n",
    "\n",
    "Advantages: Generates multiple imputed datasets, considering the uncertainty associated with missing values. Provides more accurate estimates and standard errors.\n",
    "Disadvantages: Computationally intensive. Requires assumptions about the distribution of missing data.\n",
    "Machine Learning-Based Imputation:\n",
    "\n",
    "Advantages: Utilizes machine learning models to predict missing values based on other variables. Can capture complex relationships.\n",
    "Disadvantages: Requires more computational resources. Performance depends on the quality and quantity of the available data.\n",
    "The choice of imputation technique depends on the nature of the data and the reasons for missingness. It's important to carefully consider the assumptions and potential biases introduced by each method. Additionally, evaluating the impact of imputation on the performance of subsequent analyses or models is essential to ensure the validity of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e82a88-fd98-46b4-ab2e-feeb1146f0a8",
   "metadata": {},
   "source": [
    "Q3. What are the key factors that affect students' performance in exams? How would you go about\n",
    "analyzing these factors using statistical techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357e311-5d75-4d23-9afd-44af669ed731",
   "metadata": {},
   "outputs": [],
   "source": [
    "Students' performance in exams is influenced by a multitude of factors, and analyzing these factors requires a comprehensive approach. Some key factors that may affect students' performance include:\n",
    "\n",
    "Study Habits:\n",
    "\n",
    "The amount of time spent studying, study methods, and the effectiveness of study habits.\n",
    "Attendance:\n",
    "\n",
    "Regular attendance in classes and engagement with course material.\n",
    "Prior Knowledge:\n",
    "\n",
    "The students' background knowledge and understanding of prerequisite concepts.\n",
    "Motivation:\n",
    "\n",
    "Intrinsic motivation, interest in the subject matter, and the importance students place on academic success.\n",
    "Teacher Quality:\n",
    "\n",
    "The effectiveness of teaching methods, clarity of explanations, and teacher-student interactions.\n",
    "Parental Support:\n",
    "\n",
    "The level of support and encouragement received from parents or guardians.\n",
    "Health and Well-being:\n",
    "\n",
    "Physical and mental health, as well as overall well-being, can impact concentration and focus.\n",
    "Test Anxiety:\n",
    "\n",
    "Anxiety levels during exams, which can affect performance.\n",
    "Peer Influence:\n",
    "\n",
    "Interaction with peers, study groups, and the social environment.\n",
    "To analyze these factors using statistical techniques, you could employ various methods:\n",
    "\n",
    "Descriptive Statistics:\n",
    "\n",
    "Use descriptive statistics to summarize and describe the main features of the data. This includes measures of central tendency (mean, median) and dispersion (standard deviation, range).\n",
    "Correlation Analysis:\n",
    "\n",
    "Conduct correlation analysis to explore the relationships between different variables. For example, you can examine the correlation between study hours and exam scores or attendance and performance.\n",
    "Regression Analysis:\n",
    "\n",
    "Perform regression analysis to model the relationship between a dependent variable (exam scores) and one or more independent variables (study hours, attendance, etc.). This can help identify the factors that have a significant impact on performance.\n",
    "ANOVA (Analysis of Variance):\n",
    "\n",
    "Use ANOVA to compare means across different groups. For instance, you could analyze if there are significant differences in exam scores between students with different levels of motivation or parental support.\n",
    "Factor Analysis:\n",
    "\n",
    "Apply factor analysis to identify underlying factors that may be influencing students' performance. This technique helps to group related variables and understand the latent constructs affecting outcomes.\n",
    "Logistic Regression:\n",
    "\n",
    "If you are dealing with binary outcomes (e.g., pass/fail), logistic regression can help analyze the impact of various factors on the likelihood of success.\n",
    "Machine Learning Models:\n",
    "\n",
    "Utilize machine learning models for predictive analysis. This involves training models on historical data to predict future outcomes based on various factors.\n",
    "Qualitative Analysis:\n",
    "\n",
    "Combine statistical techniques with qualitative methods such as interviews or surveys to gain a deeper understanding of students' experiences and perceptions.\n",
    "It's important to approach the analysis with care, considering the limitations of statistical techniques and the complexity of human behavior. Additionally, ethical considerations should be taken into account when analyzing and interpreting data related to students' performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3676b-2144-4873-a7de-bbfc2a001955",
   "metadata": {},
   "source": [
    "Q4. Describe the process of feature engineering in the context of the student performance data set. How\n",
    "did you select and transform the variables for your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fc6f0-f55d-4860-97c5-90b811461dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature engineering is a crucial step in the process of preparing data for machine learning models. It involves selecting, transforming, and creating features (variables) that enhance the performance of the model. In the context of a student performance dataset, feature engineering aims to improve the predictive power of the model by extracting meaningful information from the available variables. Here is a general outline of the feature engineering process:\n",
    "\n",
    "Variable Selection:\n",
    "\n",
    "Identify the variables in the dataset that are relevant to predicting student performance. This may include demographic information, study habits, attendance, and other factors mentioned in the dataset.\n",
    "Handling Missing Data:\n",
    "\n",
    "Assess and address missing values in the dataset. Depending on the extent of missing data, you may choose to drop observations, impute missing values using statistical measures (mean, median, mode), or employ more advanced imputation techniques.\n",
    "Encoding Categorical Variables:\n",
    "\n",
    "Convert categorical variables into a numerical format that can be used by machine learning models. This might involve one-hot encoding, label encoding, or other methods based on the nature of the data.\n",
    "Creating New Features:\n",
    "\n",
    "Derive new features from existing ones that may better capture relationships or patterns in the data. For example, you could calculate a \"study time per day\" feature by dividing total study time by the number of days.\n",
    "Scaling and Normalization:\n",
    "\n",
    "Ensure that numerical features are on similar scales to prevent certain features from dominating others during model training. Techniques like Min-Max scaling or standardization (z-score normalization) can be applied.\n",
    "Handling Outliers:\n",
    "\n",
    "Identify and handle outliers in the data that might adversely impact the model's performance. This could involve removing outliers or transforming the data to make it more robust to extreme values.\n",
    "Feature Interaction:\n",
    "\n",
    "Explore interactions between features and consider adding interaction terms to the dataset. For instance, if attendance and study time independently influence performance, a feature that represents their interaction might provide additional insights.\n",
    "Binning or Discretization:\n",
    "\n",
    "Convert continuous variables into discrete bins if necessary. This can be particularly useful when there is non-linear behavior in the data, and the model may benefit from categorical representations.\n",
    "Dimensionality Reduction:\n",
    "\n",
    "Consider applying dimensionality reduction techniques, such as Principal Component Analysis (PCA) or feature selection methods, to reduce the number of features while retaining relevant information.\n",
    "Time-Based Features:\n",
    "\n",
    "If the dataset includes a temporal aspect, create features related to time, such as semester or academic year indicators. This can help capture trends or seasonality in student performance.\n",
    "The specific variables chosen and the transformations applied depend on the characteristics of the dataset and the goals of the analysis. The goal is to create a set of features that provides the model with the most relevant and informative input for predicting student performance. It's often an iterative process that involves experimenting with different transformations and evaluating their impact on model performance using techniques like cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdcb66-33f1-4b8d-a996-50f914b707fa",
   "metadata": {},
   "source": [
    "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
    "of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
    "these features to improve normality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e4508-a972-4dfb-80e9-cd1d7a610f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Load the wine quality dataset (assuming it's in a CSV file)\n",
    "wine_data = pd.read_csv(\"wine_quality_dataset.csv\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(wine_data.info())\n",
    "\n",
    "# Summary statistics\n",
    "print(wine_data.describe())\n",
    "\n",
    "# Distribution plots for each feature\n",
    "plt.figure(figsize=(12, 8))\n",
    "for column in wine_data.columns:\n",
    "    plt.subplot(3, 4, wine_data.columns.get_loc(column) + 1)\n",
    "    sns.histplot(wine_data[column], kde=True)\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test for normality\n",
    "for column in wine_data.columns:\n",
    "    stat, p_value = shapiro(wine_data[column])\n",
    "    print(f\"{column}: Statistic={stat:.3f}, p-value={p_value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8396a74-8d59-48c9-89ea-e14c38ae6602",
   "metadata": {},
   "source": [
    "Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\n",
    "features. What is the minimum number of principal components required to explain 90% of the variance in\n",
    "the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99724be6-8b89-4edb-ab65-2cc19dedf343",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wine_quality_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the wine quality dataset (assuming it's in a CSV file)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m wine_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwine_quality_dataset.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Separate features and target variable\u001b[39;00m\n\u001b[1;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m wine_data\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquality\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming 'quality' is the target variable\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wine_quality_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the wine quality dataset (assuming it's in a CSV file)\n",
    "wine_data = pd.read_csv(\"wine_quality_dataset.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = wine_data.drop('quality', axis=1)  # Assuming 'quality' is the target variable\n",
    "y = wine_data['quality']\n",
    "\n",
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "plt.show()\n",
    "\n",
    "# Find the minimum number of components to explain 90% of the variance\n",
    "min_components = len(cumulative_explained_variance[cumulative_explained_variance >= 0.9])\n",
    "\n",
    "print(f\"Minimum number of principal components to explain 90% of the variance: {min_components}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
